<!DOCTYPE html>
<html lang="default">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"drawwon.github.io","root":"/","scheme":"Muse","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="这篇博文主要讲的是关于deeplearning.ai的第二门课程的内容，《Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization》">
<meta property="og:type" content="article">
<meta property="og:title" content="Coursera-deeplearning-ai-（二）">
<meta property="og:url" content="http://drawwon.github.io/2018/04/11/Coursera-deeplearning-ai-%EF%BC%88%E4%BA%8C%EF%BC%89/index.html">
<meta property="og:site_name" content="WangZhao&#39;s Blog">
<meta property="og:description" content="这篇博文主要讲的是关于deeplearning.ai的第二门课程的内容，《Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization》">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/88387245.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/22424626.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/29396365.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/27719103.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/82915442.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/98592108.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/41548130.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/1978794.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-12/9225476.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/20190225112424.png">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-12/31073973.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-28/23613139.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-28/12421014.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-30/72186468.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-1/63473191.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-2/83045151.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-2/7255551.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-2/48630632.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-2/46166279.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-2/70350494.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/80735041.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/20607262.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/36826086.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/5744779.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/5343372.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/57297767.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/89577231.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/60312042.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/75203372.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/83926609.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-4/69914394.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-4/92497288.jpg">
<meta property="article:published_time" content="2018-04-11T02:03:47.000Z">
<meta property="article:modified_time" content="2019-02-27T05:31:34.227Z">
<meta property="article:author" content="Jeffrey Pacino">
<meta property="article:tag" content="deeplearning">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/88387245.jpg">

<link rel="canonical" href="http://drawwon.github.io/2018/04/11/Coursera-deeplearning-ai-%EF%BC%88%E4%BA%8C%EF%BC%89/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true
  };
</script>

  <title>Coursera-deeplearning-ai-（二） | WangZhao's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">WangZhao's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">It's not who you are underneath,it's what you do that defines you</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="search-pop-overlay">
  <div class="popup search-popup">
      <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

  </div>
</div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="default">
    <link itemprop="mainEntityOfPage" href="http://drawwon.github.io/2018/04/11/Coursera-deeplearning-ai-%EF%BC%88%E4%BA%8C%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jeffrey Pacino">
      <meta itemprop="description" content="Humble to the dust and then out of the flowers">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WangZhao's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Coursera-deeplearning-ai-（二）
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-04-11 10:03:47" itemprop="dateCreated datePublished" datetime="2018-04-11T10:03:47+08:00">2018-04-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-02-27 13:31:34" itemprop="dateModified" datetime="2019-02-27T13:31:34+08:00">2019-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>这篇博文主要讲的是关于deeplearning.ai的第二门课程的内容，《Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization》</p>
<a id="more"></a>
<h2 id="week-one">Week one</h2>
<h3 id="设置训练验证测试集">设置训练，验证，测试集</h3>
<p>设置神经网络时，有很多的值需要你自己设置，比如隐藏层的数量，隐藏点的个数，学习率，激活函数的类型等等……</p>
<p>数据通常被分为三部分：训练集，hold-out交叉验证集（或者成为开发集dev），测试集。分布如下图所示：</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/88387245.jpg" /></p>
<p>多年前，数据较少：70%的训练数据和30%的测试数据，或者60%训练数据，验证集和测试集各占20%</p>
<p>但现在数据越来越多，100w的总数据，验证集和测试集可能都只需要1w个就行了，剩下的98w数据都可以用于训练，比例为98/1/1</p>
<p>数据更多的时候，可能开发集和测试集所占的比例更小</p>
<h4 id="数据不平衡">数据不平衡</h4>
<p>训练集，开发集，测试集的数据分布不同，比如图片识别中，两边的数据来源不同（一边是高清图片，一边是模糊图片），这时候只需要保证<strong>开发集和测试集在同一个分布</strong>即可。</p>
<h3 id="偏差方差">偏差方差</h3>
<p>深度学习中有一个问题叫做“偏差-方差困境”，要在偏差和方差之间权衡</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/22424626.jpg" /></p>
<h4 id="如何判断是高方差还是高偏差">如何判断是高方差还是高偏差</h4>
<p>往往通过训练集误差和开发集误差的对比来进行判断：</p>
<ul>
<li>训练集误差小，开发集误差大，证明过拟合了，高方差</li>
<li>训练集误差大，开发集误差约等于训练集误差，证明欠拟合，高偏差</li>
<li>训练集误差大，开发集误差远大于训练集，证明高偏差且高方差，这是因为在某些数据上过拟合，而在大部分数据上欠拟合</li>
<li>训练集误差小，开发集误差也很小，这就是最理想的状态</li>
</ul>
<p>下面这个分类猫的例子比较直观解释了上面四种情况，注意，此时所谓的大小是因为我们设置的贝叶斯先验错误为0%，所以认为1%小，15%大。如果贝叶斯先验概率不是0%而是15%，那么15%的错误率也是很小的了。并且此时要求训练集和开发集的数据分布是相同的（如果一个是高质量数据，一个是低质量数据，那么两个本来错误率就不一样）。</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/29396365.jpg" /></p>
<h3 id="机器学习的基本准则">机器学习的基本准则</h3>
<p>训练好模型之后：</p>
<ul>
<li>首先询问，是否存在<strong>高偏差</strong>（在训练集上面的表现），如果存在，那么你可以尝试使用<strong>更大的网络</strong>（更多层和更多隐藏点），或者尝试训练<strong>更多的迭代次数</strong>。尝试多种方法，直到将偏差减小到一个可以接受的范围。</li>
<li>再看看是否有较高的<strong>方差</strong>（在开发集上面的表现），如果存在，那么比较好的办法就是<strong>增加训练数据</strong>，或者是<strong>正则化</strong></li>
</ul>
<h3 id="正则化">正则化</h3>
<p>如果发现<strong>过拟合</strong>，那么就是方差过大，首先应该尝试的方法就是正则化</p>
<p>以逻辑回归为例，为了最小化代价函数<span class="math inline">\(J(W,b)=\frac{1}{m}\sum_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)})\)</span>，我们在后面加上一个W的范数，常用的范数为二范数，代价函数变为：</p>
<p><span class="math inline">\(J(W,b)=\frac{1}{m}\sum_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}||w||^{2}_{2}\)</span></p>
<p>其中的<span class="math inline">\(\lambda\)</span>是正则化参数，<span class="math inline">\(||w||^{2}_{2}\)</span>称为w的二范数，<span class="math inline">\(||w||^{2}_{2}=\sum_{j=1}^{n_{x}}w_j^2=w^Tw\)</span></p>
<p>为什么只对w正则化而忽略b呢，这是因为在过拟合的情况下，w的维度非常大，而b只有一个参数，影响相对于w来说可以忽略</p>
<p>偶尔也用一范数，但很少用，具体的逻辑回归的正则化方法如下</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/27719103.jpg" /></p>
<h4 id="神经网络的正则化">神经网络的正则化</h4>
<p>神经网路的正则化的方法和逻辑回归基本一样，只是w的二范数成了w矩阵的元素平方和，这个值被称为Frobenius norm（弗罗贝尼乌斯范数）</p>
<p>在反向传播的时候，反向传播的<span class="math inline">\(dw^{[l]}\)</span>就成了原本的反向传播的值（下图中间绿色方框行，由代价函数J求导得到），加上<span class="math inline">\(\frac{\lambda}{m}w^{[l]}\)</span>，w的更新公式就成了这样:</p>
<p><span class="math inline">\(w^{[l]}=(1-\frac{\alpha\lambda}{m})w^{[l]}-\alpha(原本的反向传播值)\)</span></p>
<p>所以正则化之后，每次更新相当于只是在原本的w前面乘以一个略小于1的值<span class="math inline">\(1-\frac{\alpha\lambda}{m}\)</span>，再减去原本的反向传播的值，因此神经网络的正则化又被称之为权重衰减</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/82915442.jpg" /></p>
<h3 id="为什么正则化可以消除过拟合">为什么正则化可以消除过拟合</h3>
<p>如图，如果过拟合，我们在加入正则化之后，如果把<span class="math inline">\(\lambda\)</span>设置的非常大，那么为了是代价函数最小，w必须非常小，那么w的很多值就为0了，多层神经网络看上去就像是一个简单神经网路一样</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/98592108.jpg" /></p>
<p>另一个直观解释是当你使用tanh之类的激活函数的时候，当<span class="math inline">\(\lambda\)</span>非常大的时候，那么w非常小，因此z也非常小，经过激活函数变化之后的a也非常小，因此a值只能在0附近变化，这一段tanh函数基本相当于一个线性函数，也就是多层神经网络变化之后基本相当于在做线性变换，就变成一个接近线性变化的值</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/41548130.jpg" /></p>
<h3 id="dropout-正则化">dropout 正则化</h3>
<p>dropout正则化，也就是丢弃法正则化，也成为随机失活正则化。</p>
<p>对每个点进行抛硬币，50%的概率丢弃该点，得到一个丢弃一部分的神经网络，这个方法虽然听上去不可靠，但是实际表现却不错</p>
<p>随机失活正则化的实现方法：</p>
<p>假设有一个L=3的神经网络，先设置一个保留率keep-prob，随机产生一个3*n的矩阵，与keep-prob比较之后产生d3，让原本的w乘以这个d3，再除以一个keep-prob以消除引入随机失活的影响（因为你引入随机失活，相当于对某层的a乘以一个keep-prob，那么我要结果一样，就要除以一个keep-prob）</p>
<p>举个例子为什么要除以keep-prob，比如我们现在第三层有50个点，如果keep-prob为0.8的话，那么这层大概平均来说有10个点要失效，<span class="math inline">\(z^{[4]}=w^{[4]}a^{[3]}+b^{[4]}\)</span>，那么此时的<span class="math inline">\(a^{[3]}\)</span>的期望就变成了原来的80%，为了使得<span class="math inline">\(z^{[4]}\)</span>的期望不变，我们就需要将<span class="math inline">\(a^{[3]}\)</span>除以一个keep-prob来确保<span class="math inline">\(z^{[4]}\)</span>期望不变。</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/1978794.jpg" /></p>
<h3 id="随机失活正则化的理解">随机失活正则化的理解</h3>
<p>直观解释：因为你不知道哪一个神经元可能被丢弃，所以你不能过分依赖某个神经元，因此权重就不得不分散</p>
<p>在真正使用的时候，如果你担心某层容易过拟合，那么就把这一层的留存率设置的低一些；如果确认不会过拟合，那就把留存率设置接近1，比如在输入层这里留存率就应该是1</p>
<h3 id="其它正则化方法">其它正则化方法</h3>
<p>在图片处理的时候，如果你没有更多的数据，比如处理猫之类的：你可以将图片进行水平翻转，或者放大旋转之类的，处理数字的时候：可以扭曲加旋转</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-12/9225476.jpg" /></p>
<p>另一种方法叫做<strong>早终止方法</strong>（early stopping）</p>
<p>画出训练集的代价函数和开发集的代价函数，选择两者都还比较小的值</p>
<h3 id="归一化normalization">归一化（normalization）</h3>
<p>归一化可以加速训练过程</p>
<p>归一化的过程：<strong>减去均值（<span class="math inline">\(x-\mu\)</span>），将方差归一化<span class="math inline">\((x-\mu)/\sigma\)</span></strong></p>
<p>归一化过程中一定要注意，对训练集和测试集都需要归一化</p>
<h3 id="梯度消失和梯度爆炸">梯度消失和梯度爆炸</h3>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/20190225112424.png" /></p>
<p>由于：</p>
<p><span class="math display">\[a^{[l]}=\sigma(w^{[l]}a^{[l-1]}+b^{[l]}) \tag{1}\]</span></p>
<p><span class="math display">\[a^{[l-1]}=\sigma(w^{[l-1]}a^{[l-2]}+b^{[l-1]}) \tag{2}\]</span></p>
<p><span class="math display">\[a^{[l-2]}=\sigma(w^{[l-2]}a^{[l-3]}+b^{[l-2]}) \tag{3}\]</span></p>
<p>求导可得</p>
<p><span class="math inline">\(dz^{[l]}=da^{[l]} * {g^{[l]}}&#39;(z^{[l]}) \tag{4}​\)</span></p>
<p>$dw<sup>{[l]}=dz</sup>{[l]}*a^{[l-1]} $</p>
<p><span class="math inline">\(db^{[l]}=dz^{[l]}\tag{6}​\)</span></p>
<p>接着求前一层：</p>
<p>由上面公式(1)可以得到：</p>
<p><span class="math inline">\(da^{[l-1]}=dz^{[l]}*w^{[l]}\tag{7}​\)</span></p>
<p>由公式(2)得到：</p>
<p><span class="math inline">\(dz^{[l-1]}=da^{[l-1]} * {g^{[l-1]}}&#39;(z^{[l-1]}) \tag{8}​\)</span></p>
<p><span class="math inline">\(dw^{[l-1]}=dz^{[l-1]}*a^{[l-2]} \tag{9}​\)</span></p>
<p>结合(7),(8),(9)得到：</p>
<p><span class="math inline">\(dw^{[l-1]}=dz^{[l]}*w^{[l]}* {g^{[l-1]}}&#39;(z^{[l-1]}) \tag{10}\)</span></p>
<p>继续对(3)进行求导：</p>
<p><span class="math inline">\(dz^{[l-2]}=da^{[l-2]} * {g^{[l-2]}}&#39;(z^{[l-2]}) \tag{11}​\)</span></p>
<p><span class="math inline">\(dw^{[l-2]}=dz^{[l-2]}*a^{[l-3]} \tag{12}\)</span></p>
<p>由公式(2)得到：</p>
<p><span class="math inline">\(da^{[l-2]}=dz^{[l-1]}*w^{[l-1]}\tag{13}​\)</span></p>
<p>结合(11),(12),(13)得到：</p>
<p><span class="math inline">\(dw^{[l-2]}=dz^{[l-1]}*w^{[l-1]}*{g^{[l-2]}}&#39;(z^{[l-2]})*a^{[l-3]}​\)</span></p>
<p>再结合(4),(7),(8)可得</p>
<p><span class="math inline">\(dw^{[l-2]}=da^{[l]}*w^{[l]}*w^{[l-1]}*{g^{[l]}}&#39;(z^{[l]})*{g^{[l-1]}}&#39;(z^{[l-1]}) *{g^{[l-2]}}&#39;(z^{[l-2]})*a^{[l-3]}\)</span></p>
<p>比如你的激活函数是g(z)=z，损失函数是交叉熵函数，<span class="math inline">\(da=a-y​\)</span>然后<span class="math inline">\(dw^{[1]}=w^{[l]}\times w^{[l-1]}\times...\times w^{[2]}\times w^{[1]}\times X​\)</span>，只要所有w都是对角矩阵，他的某一项大于1，则出现梯度爆炸，求出的梯度非常大，或者是梯度消失，求出的梯度基本为0</p>
<h3 id="权重初始化和深度网络">权重初始化和深度网络</h3>
<p>特殊地初始化可以部分解决梯度爆炸和梯度消失的问题</p>
<p>在使用Relu激活函数的时候：</p>
<p><span class="math inline">\(W^{[L]}=np.random.randn(shape) * np.sqrt(1/n)​\)</span></p>
<h3 id="梯度检验">梯度检验</h3>
<p>根据导数的定义，对代价函数进行求导：</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-12/31073973.jpg" /></p>
<p>检查：两个导数之间的欧式距离/两个导数的2范数之和，如果基本等于<span class="math inline">\(\epsilon\)</span>的话，那就说明正确了，如果大于<span class="math inline">\(\epsilon\)</span>很多的话，就说明错了</p>
<h3 id="梯度下降的实现">梯度下降的实现</h3>
<ul>
<li>只在调试的时候用提督检验，在训练的时候不要用</li>
<li>如果算法梯度检验失败，检查每一个dw，db来找到程序的bug</li>
<li>记得正则化</li>
<li>在没有dropout的时候先进行梯度检验，发现算法没有问题再使用dropout</li>
<li>随机初始化可以先运行一下梯度检验</li>
</ul>
<h3 id="合适的初始化方法">合适的初始化方法</h3>
<p>He初始化方法（<a href="https://arxiv.org/abs/1502.01852">He et al., 2015</a>），在激活函数是Relu的时候非常有效，具体做法是<span class="math inline">\(W^{[l]}=\rm{np.random.randn}(layer\_dimension[l],layer\_dimension[l-1])*\rm{np.sqrt}(2./layer\_dimension[l-1])\)</span></p>
<h2 id="第二周">第二周</h2>
<h3 id="优化算法">优化算法</h3>
<p>向量化可以高效计算m个example，但是当example非常多的时候，计算起来也是非常的慢的，比如你现在有500w个example，拿计算起来就是非常慢的</p>
<p>为了加快计算的速度，提出了mini-batch gradient descent，也就是批量梯度下降，将数据分成一个个的小batch，然后进行前向传播，反向传播，参数更新等步骤，这样计算速度会快上很多</p>
<p>比如现在有500w条数据，将每1000条数据凑成一个batch，用<code>{}</code>来表示第多少个batch，现在分成了<span class="math inline">\(X^{\{1\}}\)</span>到<span class="math inline">\(X^{\{5000\}}\)</span>共5000个batch，每个batch的维度是<span class="math inline">\((n_x,1000)\)</span></p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-28/23613139.jpg" /></p>
<p>Y以同样的方法被分成5000份，每个<span class="math inline">\(Y^{\{t\}}\)</span>的维度是(1，1000)</p>
<p>到目前为止，我们一共用过三种括号，分别是小括号，中括号，和大括号</p>
<ul>
<li>小括号：<span class="math inline">\(x^{(i)}\)</span>，表示第i个训练实例</li>
<li>中括号：<span class="math inline">\(Z^{[L]}\)</span>表示第L层</li>
<li>大括号：<span class="math inline">\(X^{\{t\}}\)</span>,<span class="math inline">\(Y^{\{t\}}\)</span>表示第t个batch</li>
</ul>
<p>分成batch之后的步骤和之前的神经网络的构建步骤一样，只是多了一重循环batch的for</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-28/12421014.jpg" /></p>
<h3 id="理解mini-batch梯度下降">理解mini-batch梯度下降</h3>
<p>批量梯度下降的损失函数往往一直下降，但是mini-batch梯度下降存在噪声，但是整体趋势是下降的</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-30/72186468.jpg" /></p>
<p>两种极端情况：</p>
<ol type="1">
<li>如果mini-batch的size=m，那么这就是梯度下降，梯度下降的好处是每一步迭代都是往最优值的方向去靠近，但是数据量很大的时候，批量梯度下降就会非常的慢，这种情况又被称为批梯度下降</li>
<li>如果mini-batch的size=1，那么这种情况就是每次输入一个example，这样每次迭代的方向可能是乱的，最终的结果可能在最优值附近徘徊，这种情况又被称为随机梯度下降</li>
<li>只有mini-batch值合适的时候，才能既用到向量化的加速运算，又能得到一个最优值</li>
</ol>
<p>一般认为：</p>
<p>在m&lt;=2000时，认为数据量足够下，可以使用批量梯度下降</p>
<p>在m&gt;2000时，通常使用的mini-batch的size为64, 128, 256, 512，用2的倍数是因为内存读取的方式是通过2的倍数来读取的，这样能够加快运算</p>
<h3 id="指数加权平均">指数加权平均</h3>
<p>如图，是一大堆温度数据，我们为了对温度数据做个平均，用v0=0,<span class="math inline">\(v_1=0.9v_0+0.1\theta_1\)</span>，一直到<span class="math inline">\(v_t=0.9v_{t-1}+0.1\theta_t\)</span>进行指数加权平均</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-1/63473191.jpg" /></p>
<p>这种指数加权平均的效果的<span class="math inline">\(v_{t}\)</span>就大致等同于对<span class="math inline">\(\frac{1}{1-\beta}\)</span>天的数据进行平均，其中<span class="math inline">\(\beta\)</span>是<span class="math inline">\(v_t=\beta v_{t-1}+(1-\beta)\theta_t\)</span>这个公式中的系数</p>
<p>比如，当<span class="math inline">\(\beta=0.9\)</span>时，这就相当于对前10天的数据进行平均；当的<span class="math inline">\(\beta=0.98\)</span>时，这就相当于对前50天的数据进行平均；当的<span class="math inline">\(\beta=0.5\)</span>时，这就相当于对前2天的数据进行平均</p>
<p>更大的<span class="math inline">\(\beta\)</span>意味着更平滑的曲线，但是对数据的延迟性也更大</p>
<h3 id="指数加权平均的理解">指数加权平均的理解</h3>
<p>通用的迭代公式：$v_t=v_{t-1}+(1-)_t $</p>
<p>我们来举个例子，假如<span class="math inline">\(\beta=0.9\)</span></p>
<p>那么</p>
<p><span class="math inline">\(v_{100}=0.9 v_{99}+0.1\theta_{100}\)</span></p>
<p><span class="math inline">\(v_{99}=0.9 v_{98}+0.1\theta_{99}\)</span></p>
<p><span class="math inline">\(v_{98}=0.9 v_{97}+0.1\theta_{98}\)</span></p>
<p>将<span class="math inline">\(v_{100}=0.9 v_{99}+0.1\theta_{100}\)</span>展开可以得到</p>
<p><span class="math inline">\(v_{100}=0.9 v_{99}+0.1\theta_{100}=0.1\theta_{100}+0.9(0.1\theta_{99}+0.9 v_{98})=0.1\theta_{100}+0.9*0.1\theta_{99}+0.9^2(0.9 v_{97}+0.1\theta_{98})...\)</span></p>
<p>这个过程与我们平时的平均数有类似的地方，因为我们平时求解的平均数是在每个<span class="math inline">\(\theta\)</span>前面的系数相等，都是1/n，在指数加权平均的时候，将靠的近的系数放大，靠的远的系数变小，以指数形式衰减</p>
<p>这样下去，要使得v的加和的那一项足够小， 也就是<span class="math inline">\(0.1*0.9^{t}\)</span>足够小的情况下，<span class="math inline">\(0.9^{10}=1/e\)</span>，就认为是10天的平均</p>
<p><strong>指数加权平均的好处：</strong></p>
<p>我们可以看到指数加权平均的求解过程实际上是一个递推的过程，那么这样就会有一个非常大的好处，每当我要求从0到某一时刻（n）的平均值的时候，我并不需要像普通求解平均值的作为，保留所有的时刻值，类和然后除以n。</p>
<p>而是只需要保留0-(n-1)时刻的平均值和n时刻的温度值即可。也就是每次只需要保留常数值，然后进行运算即可，这对于深度学习中的海量数据来说，是一个很好的减少内存和空间的做法。</p>
<h3 id="偏差修正">偏差修正</h3>
<p>因为<span class="math inline">\(v_0=0\)</span>，而<span class="math inline">\(v_1=0.98v_0+0.02\theta_1\)</span>，因为<span class="math inline">\(v_0=0\)</span>，所以<span class="math inline">\(v_1=0.02\theta_1\)</span>；<span class="math inline">\(v_2=0.98v_1+0.02\theta_2\)</span>，<span class="math inline">\(v_2=0.0196\theta_1+0.02\theta_2\)</span></p>
<p>由于上面两个等式展现的原因，这些v的值在初始阶段都很小，为了使这些初始阶段的值可以作为平均，我们用<span class="math inline">\(v_t=\frac{v_t}{1-\beta^t}\)</span>来进行偏差修正，如下图</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-2/83045151.jpg" /></p>
<h3 id="动量momentum梯度下降">动量(Momentum)梯度下降</h3>
<p>动量梯度下降比普通的梯度下降更快，其主要思想是：计算梯度的指数加权平均，使用这个梯度来更新权重</p>
<p>实现的方式如下，<span class="math inline">\(\beta\)</span>参数最常用的值就是0.9：</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-2/7255551.jpg" /></p>
<p>进行动量梯度下降之后，纵轴上的偏差被减小了，得到如下图红线的效果</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-2/48630632.jpg" /></p>
<h3 id="rmsproproot-mean-square-prop算法">RMSprop(Root Mean Square prop)算法</h3>
<p>实现的方法和momentum类似，但是公式变成了</p>
<p><span class="math inline">\(S_{dw}=\beta_2S_{dw}+(1-\beta_2)dw^{2}\)</span></p>
<p><span class="math inline">\(S_{db}=\beta_2S_{db}+(1-\beta_2)db^{2}\)</span></p>
<p>而迭代公式变成了</p>
<p><span class="math inline">\(w:=w-\alpha\frac{dw}{\sqrt{S_{dw}}+\epsilon}\)</span></p>
<p><span class="math inline">\(b:=b-\alpha\frac{dw}{\sqrt{S_{db}}+\epsilon}\)</span></p>
<p>加一个<span class="math inline">\(\epsilon\)</span>是为了不出现除以0的情况</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-2/46166279.jpg" /></p>
<h3 id="adamadaptive-moment-estimation-优化算法">Adam(Adaptive moment estimation) 优化算法</h3>
<p>Adam(Adaptive moment estimation)的意思是：适应性矩优化，这里的矩指的是一阶矩，二阶矩那个矩。</p>
<p>Adam就是将momentum和RMSprop结合起来</p>
<p>实现方法如下图，注意这里的参数都需要修正偏差：</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-2/70350494.jpg" /></p>
<p>里面的超参，一般来说momentum的超参<span class="math inline">\(\beta_1=0.9\)</span>，RMSprop的超参<span class="math inline">\(\beta_2=0.999\)</span>，<span class="math inline">\(\epsilon=10^{-8}\)</span>，学习率<span class="math inline">\(\alpha\)</span> 是需要去调整的参数，Adam的公式如下，将w换成b则得到b的更新公式</p>
$$
<span class="math display">\[\begin{cases}
v_{dW^{[l]}} = \beta_1 v_{dW^{[l]}} + (1 - \beta_1) \frac{\partial \mathcal{J} }{ \partial W^{[l]} } \\

v^{corrected}_{dW^{[l]}} = \frac{v_{dW^{[l]}}}{1 - (\beta_1)^t} \\
s_{dW^{[l]}} = \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial W^{[l]} })^2 \\
s^{corrected}_{dW^{[l]}} = \frac{s_{dW^{[l]}}}{1 - (\beta_2)^t} \\
W^{[l]} = W^{[l]} - \alpha \frac{v^{corrected}_{dW^{[l]}}}{\sqrt{s^{corrected}_{dW^{[l]}}} + \varepsilon}
\end{cases}\]</span>
<p>$$</p>
<h3 id="学习率衰减">学习率衰减</h3>
<p>我们用下面的公式来衰减学习率<span class="math inline">\(\alpha\)</span>：</p>
<p><span class="math inline">\(\alpha=\frac{1}{1+decay\_rate\times epoch\_num}\alpha_0\)</span></p>
<p>decay_rate是这里的下降率，epoch_num是迭代的次数</p>
<h3 id="局部最优解">局部最优解</h3>
<p>在二维图像中，很容易产生局部最优解，但是在高维的时候，你要找到一个这个点在所有维度上梯度都为0，这是非常困难的，我们称这种有部分维度梯度为0的点为鞍点，因为图形的形状就好像马鞍一样</p>
<h3 id="week-3">Week 3</h3>
<h2 id="batch-normalization">Batch Normalization</h2>
<h3 id="调参过程">调参过程</h3>
<p>神经网络有很多的超参，调整超参有利于改进神经网络的性能</p>
<p>参数有很多，包括：学习率<span class="math inline">\(\alpha\)</span>，momentum当中的<span class="math inline">\(\beta\)</span>，Adam优化中的<span class="math inline">\(\beta_1,\beta_2,\epsilon\)</span>，网络层数，隐藏单元，学习率衰减方式，mini-batch的size</p>
<p>一般来说需要调整的重要程度排序为：</p>
<p><span class="math inline">\(\alpha&gt;\rm{momentum当中的}\beta=mini-batch\ size=隐藏单元数量&gt;网络层数&gt;学习率衰减参数&gt;&gt;Adam（Adam一般不调参，用默认参数\beta_1=0.9,\beta_2=0.999,\epsilon=10^{-8}）\)</span></p>
<p>但这并不是一个死板的规定，可能有其他的规则</p>
<p>早期调参的时候，通常是启发式搜索，然后给定最优的参数；参数很多的时候，建议随机选择点，进行尝试，如下图右边</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/80735041.jpg" /></p>
<p>当你能确定更小的范围的时候，就可以在这个范围内进行更加密集的搜索，直到找到你能接受的最优参数</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/20607262.jpg" /></p>
<h3 id="选择合适尺度去选取超参数">选择合适尺度去选取超参数</h3>
<p>很多超参数是不能在某个范围内均匀取样的，比如考虑学习率<span class="math inline">\(\alpha\)</span>，让<span class="math inline">\(\alpha\)</span>从0.0001到1取值，肯定要求在0.0001到0.001之前取多点，而0.1-1之间要比较少，所以我们此时用到对数的取法，也就是从10e-4取到10e0，那我们就只需要去一个-4到0的随机数，用a = -4 * np.random.randn(), alpha=10**a</p>
<p>还有比如momentum当中的<span class="math inline">\(\beta\)</span>参数，如果让<span class="math inline">\(\beta\)</span>从0.9取到0.999，在靠近0.999的时候，稍微改变一点点都会让平均值的范围变化很大，因此在后面我们要取的密集一些，我们考虑<span class="math inline">\(1-\beta\)</span>，<span class="math inline">\(\beta\)</span>从0.9到0.999，那么<span class="math inline">\(1-\beta\)</span>就从0.1到0.001，取一个从-3到-1的随机数，再用10的指数来代替<span class="math inline">\(1-\beta\)</span>，那么<span class="math inline">\(\beta=1-10^t\)</span></p>
<h3 id="熊猫模型和鱼子酱模型">熊猫模型和鱼子酱模型</h3>
<p>熊猫模型：关注你的模型，就如同熊猫产子一般，一次调整一点</p>
<p>鱼子酱模型：一次同时开始多个模型的训练，如同鱼类产子一般</p>
<p>计算资源足够的时候，就用鱼子酱模型，否则用熊猫模型</p>
<p>这两个名称只是为了好记忆，并没有特别的意思</p>
<h3 id="批量归一化">批量归一化</h3>
<p>在之前的归一化当中，我们只是对第一步的输入进行了归一化，但是其实每一层神经网络的输入应该都有归一化，在归一化z和a这两种选择中，业界都默认归一化z</p>
<p>对z的归一化过程如下：</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/36826086.jpg" /></p>
<p>红框部分就是归一化的过程，对于每一个z(i)，计算均值<span class="math inline">\(\mu\)</span>，方差<span class="math inline">\(\sigma^2\)</span>，然后用<span class="math inline">\(z^{(i)}_{norm}=\frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\epsilon}}\)</span>，这里加一个<span class="math inline">\(\epsilon\)</span>的原因是为了避免除以0的情况发生，然后用<span class="math inline">\(\tilde{z}^{(i)}=\gamma z^{(i)}_{norm}+\beta\)</span>，这个<span class="math inline">\(\gamma\)</span>和<span class="math inline">\(\beta\)</span>是可以从模型当中学习出来的参数。</p>
<p>为什么要用<span class="math inline">\(\gamma\)</span>和<span class="math inline">\(\beta\)</span>这两个参数呢，是因为比如你中间某一层的激活函数是sigmoid函数，如果你让你的z均值为0，方差为1，那么z的变化范围就很靠近0，这是sigmoid函数基本就成了线性函数，为了利用好sigmoid的非线性，所以对中间的z的归一化稍有不同</p>
<h3 id="将batch-norm运用到神经网络中">将batch-norm运用到神经网络中</h3>
<p>假设我们有一个如下图所示的三层神经网络，那么我们将x输入，通过w[1]和b[1]，得到z[1]，对z1进行batch-norm，通过<span class="math inline">\(\gamma^{[1]}\)</span>和<span class="math inline">\(\beta^{[1]}\)</span>得到<span class="math inline">\(\tilde{z}^{[1]}\)</span>，然后将<span class="math inline">\(\tilde{z}^{[1]}\)</span>通过g[1]得到a[1]，同理得到z[2]，<span class="math inline">\(\tilde{z}^{[2]}\)</span>，a[2]</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/5744779.jpg" /></p>
<p>此时的参数就有了w[1],b[1],w[2],b[2]，<span class="math inline">\(\gamma^{[1]}\)</span>,<span class="math inline">\(\beta^{[1]}\)</span>,<span class="math inline">\(\gamma^{[2]}\)</span>,<span class="math inline">\(\beta^{[2]}\)</span>,在TensorFlow中我们可以直接一行语句实现batch-normalization,<code>tf.nn.batch-normalization</code></p>
<p>那如何将batch-normalization用到mini-batch-normalization中呢</p>
<p>如下图，每次用一个mini-batch，对其进行batch-normalization。</p>
<p>值得注意的是，因为<span class="math inline">\(z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}\)</span>，而<span class="math inline">\(z_{norm}=\frac{z-\mu}{\sqrt{\sigma^2+\epsilon}}\)</span>，每次归一化的时候减去了均值，所以加的<span class="math inline">\(b^{[l]}\)</span>会被减掉，因此b这个参数在mini-batch-normalization时可以忽略</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/5343372.jpg" /></p>
<p>实现的具体方法，对于每一次mini-batch t，计算对于<span class="math inline">\(X^{[t]}\)</span>的前向传播，对每个隐藏层使用BN（batch-normalization）方法，然后反向传播去更新W,<span class="math inline">\(\beta\)</span>,<span class="math inline">\(\gamma\)</span>三个参数（b被减掉因此忽略），当然这里更新的方式可以是momentum，RMSprop或者Adam</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/57297767.jpg" /></p>
<h3 id="为什么batch-normalization会有效">为什么batch-normalization会有效</h3>
<p>首先，normalization会使得所有的x的值在同一个量级上面，这样能够加速迭代</p>
<p>协变量转换（covariate shift）是指在数据x变化之后，原来的网络不适用于分类新的数据的情况，如果我们使用了batch-normalization方法，前面层的变化对后面层的影响就降低了，因为被平均了，所以BN会使得系统优化的结果更好</p>
<p>同时，这还起到了一点点正则化的作用，因为每个mini-batch在计算的时候都被平均了，所以整个网络对于数据的适应性就没有那么强了</p>
<h3 id="对测试数据的batch-norm">对测试数据的batch norm</h3>
<p>在训练阶段，我们每次可以用一次批量的值计算均值和方差，但是在测试阶段，我们每次输入的只有一个值，这时候我们进行batch norm的均值和方差从哪里来呢？</p>
<p>解决办法就是，记录下训练数据的均值和方差，然后对各个mini-batch norm的均值和方差做指数权重平均，在测试阶段使用</p>
<h2 id="多分类">多分类</h2>
<h3 id="softmax-regression">softmax Regression</h3>
<p>我们之前接触的问题都是二分类，当我们要进行多分类的时候，就要用到一个特殊的激活函数，叫做softmax</p>
<p>假设我们要分类的类别数C=4，标签为0,1,2,3，那么在最后一层，我们要输出一个4*1的输出层，每一个输出点代表分到该类的概率</p>
<p>举个例子，我们得到了最后一层的输入为z[L] = [5,2,-1,3]，我们用指数函数对其变换，<span class="math inline">\(t = [e^5,e^2,e^{-1},e^3]\)</span>，计算比例得到<span class="math inline">\(a^{[L]}\)</span>，如下图所示</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/89577231.jpg" /></p>
<h3 id="对softmax的理解">对softmax的理解</h3>
<p>softmax是一个<span class="math inline">\(\frac{e^{z_j}}{\sum_ke^{z_k}}\)</span>形式的激活函数，当分类的类别C=2的时候，softmax就是logistics函数</p>
<p>softmax的loss一般取为：<span class="math inline">\(L(\hat{y},y)=-\sum_{j=1}^Cy_j\log \hat{y}_j\)</span></p>
<p>真实的y和<span class="math inline">\(\hat y\)</span>的形式如下：</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/60312042.jpg" /></p>
<p>真实值只有真的那个地方为1，别的地方为0，<span class="math inline">\(\hat y\)</span>是C个概率，代表分到每一类的概率</p>
<p>因为y一般有很多个需要分类的样本，所以真实的y和<span class="math inline">\(\hat y\)</span>如下，其中的4是此时分为4类</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/75203372.jpg" /></p>
<p>反向传播中，softmax的导数的求法稍微复杂一点，过程如下：</p>
<p>首先求<span class="math inline">\(\partial J/\partial a\)</span>，虽然这里有个累加，但是其实只有真实的那类<span class="math inline">\(y_j=1\)</span>，别的都是0，所以求和号可以去掉，变成<span class="math inline">\(J=y_j\log \hat{y}_j\)</span>，对<span class="math inline">\(\hat y\)</span>求偏导可以得到，<span class="math inline">\(\partial J/\partial a=-1/\hat{y}_j\)</span></p>
<p>接下来求softmax的导数，也就是<span class="math inline">\(\hat{y}_j\)</span>对所有的<span class="math inline">\(z_i\)</span>求导数，分为i=j和i!=j的情况来求</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/83926609.jpg" /></p>
<p>这样，<span class="math inline">\(\partial J/\partial z\)</span>的值就可以通过链式法则得到</p>
<p>当i=j时，<span class="math inline">\(\partial J/\partial z=a_j-1\)</span></p>
<p>当i!=j时，<span class="math inline">\(\partial J/\partial z=-a_i\)</span></p>
<p>在使用深度学习框架的时候，比如TensorFlow和caffe，我们只需要规划好前向传播的过程，反向传播的过程框架会自动帮你完成</p>
<h2 id="深度学习框架的介绍">深度学习框架的介绍</h2>
<p>目前主流的深度学习框架和选择标准如下：</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-4/69914394.jpg" /></p>
<h3 id="tensorflow简介">TensorFlow简介</h3>
<p>引入TensorFlow，通过<code>import tensorflow as tf</code></p>
<p>w设置为tf当中的变量，用<code>tf.Variable(initial_value=0,dtype=tf.float32)</code>表示</p>
<p>x是输入值，一开始不知道是多少，只表示dtype和shape，用<code>tf.placeholder(dtype=tf.float32,shape=[3,1])</code>表示</p>
<p>表示cost函数，因为tf已经重载了加减乘除的形式，所以可以直接用加减乘除表示，也可以用<code>tf.add</code>之类的表示，矩阵乘法的表示是<code>tf.matmul()</code></p>
<p>之后表示train的方法和目标：我们这里用梯度下降，最小化cost<code>train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)</code>，如果要用别的优化方法，只需要将<code>GradientDescentOptimizer</code>替换为别的函数就好了，括号里面的参数是learning-rate</p>
<p>然后初始化变量值，<code>init = tf.global_variables_initializer()</code></p>
<p>定义一个session，用session来run一下init，再run一下w，看看w的值，最后迭代run(train)</p>
<p>也可以用如下形式定义session</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">	session.run(init)</span><br><span class="line">    session.run(w)</span><br></pre></td></tr></table></figure>
<p>placeholder的值可以用feed_dict传入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">x = tf.placeholder(tf.int64, name = <span class="string">'x'</span>)</span><br><span class="line">print(sess.run(<span class="number">2</span> * x, feed_dict = &#123;x: <span class="number">3</span>&#125;))</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-4/92497288.jpg" /></p>
<p>写TensorFlow的代码过程大致如下：</p>
<ol type="1">
<li>建立未执行的tensor变量</li>
<li>写tensor之间的运算</li>
<li>初始化tensor</li>
<li>建立session</li>
<li>运行session，将会运行你简历里的运算</li>
</ol>
<p>所有的运算都要run之后才能执行，如果你直接print运算的话，只会得到一个tensor，也就是计算图</p>
<p>因此，请注意初始化变量，建立session并run operation</p>
<h4 id="损失计算">损失计算</h4>
<p>计算形如：</p>
<p><span class="math display">\[ J = - \frac{1}{m}  \sum_{i = 1}^m  \large ( \small y^{(i)} \log a^{ [2] (i)} + (1-y^{(i)})\log (1-a^{ [2] (i)} )\large )\small\]</span></p>
<p>这样的损失的时候，可以使用tf内置的<code>tf.nn.sigmoid_cross_entropy_with_logits</code>函数实现</p>
<h4 id="one_hot-encoding">one_hot encoding</h4>
<p>one_hot：只有一个值为1，别的值都为0的vector，用<code>tf.one_hot</code>实现，参数<code>indices</code>表示需要转换的向量, <code>depth</code>表示一共多少个类， <code>on_value=None</code>表示符合类的值为多少, <code>off_value=None</code>表示不符合类的值是多少, <code>axis</code>为0表示每个indices放一行，-1表示每个indices放一列</p>
<h4 id="实现tensorflow-model的步骤">实现TensorFlow model的步骤</h4>
<ol type="1">
<li>建立一个计算图</li>
<li>run这个计算图</li>
</ol>
<h3 id="初始化参数的方法">初始化参数的方法</h3>
<p>W用Xavier初始化，b用zero初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W1 = tf.get_variable(<span class="string">"W1"</span>, [<span class="number">25</span>,<span class="number">12288</span>], initializer = tf.contrib.layers.xavier_initializer()）</span><br><span class="line">b1 = tf.get_variable(<span class="string">"b1"</span>, [<span class="number">25</span>,<span class="number">1</span>], initializer = tf.zeros_initializer())</span><br></pre></td></tr></table></figure>
<h3 id="反向传播的方法">反向传播的方法</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#For instance, for gradient descent the optimizer would be:</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)</span><br><span class="line"><span class="comment">#To make the optimization you would do:</span></span><br><span class="line">_ , c = sess.run([optimizer, cost], feed_dict=&#123;X: minibatch_X, Y: minibatch_Y&#125;)</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/deeplearning/" rel="tag"># deeplearning</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/04/10/c++%20primer-%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/" rel="prev" title="c++ primer 基础语法">
      <i class="fa fa-chevron-left"></i> c++ primer 基础语法
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/04/16/selenium-%E7%88%AC%E5%8F%96ajax%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5/" rel="next" title="selenium 爬取ajax动态网页">
      selenium 爬取ajax动态网页 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#week-one"><span class="nav-number">1.</span> <span class="nav-text">Week one</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#设置训练验证测试集"><span class="nav-number">1.1.</span> <span class="nav-text">设置训练，验证，测试集</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#数据不平衡"><span class="nav-number">1.1.1.</span> <span class="nav-text">数据不平衡</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#偏差方差"><span class="nav-number">1.2.</span> <span class="nav-text">偏差方差</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#如何判断是高方差还是高偏差"><span class="nav-number">1.2.1.</span> <span class="nav-text">如何判断是高方差还是高偏差</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#机器学习的基本准则"><span class="nav-number">1.3.</span> <span class="nav-text">机器学习的基本准则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正则化"><span class="nav-number">1.4.</span> <span class="nav-text">正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#神经网络的正则化"><span class="nav-number">1.4.1.</span> <span class="nav-text">神经网络的正则化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么正则化可以消除过拟合"><span class="nav-number">1.5.</span> <span class="nav-text">为什么正则化可以消除过拟合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dropout-正则化"><span class="nav-number">1.6.</span> <span class="nav-text">dropout 正则化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机失活正则化的理解"><span class="nav-number">1.7.</span> <span class="nav-text">随机失活正则化的理解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#其它正则化方法"><span class="nav-number">1.8.</span> <span class="nav-text">其它正则化方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#归一化normalization"><span class="nav-number">1.9.</span> <span class="nav-text">归一化（normalization）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度消失和梯度爆炸"><span class="nav-number">1.10.</span> <span class="nav-text">梯度消失和梯度爆炸</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#权重初始化和深度网络"><span class="nav-number">1.11.</span> <span class="nav-text">权重初始化和深度网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度检验"><span class="nav-number">1.12.</span> <span class="nav-text">梯度检验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降的实现"><span class="nav-number">1.13.</span> <span class="nav-text">梯度下降的实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#合适的初始化方法"><span class="nav-number">1.14.</span> <span class="nav-text">合适的初始化方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第二周"><span class="nav-number">2.</span> <span class="nav-text">第二周</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#优化算法"><span class="nav-number">2.1.</span> <span class="nav-text">优化算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#理解mini-batch梯度下降"><span class="nav-number">2.2.</span> <span class="nav-text">理解mini-batch梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#指数加权平均"><span class="nav-number">2.3.</span> <span class="nav-text">指数加权平均</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#指数加权平均的理解"><span class="nav-number">2.4.</span> <span class="nav-text">指数加权平均的理解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#偏差修正"><span class="nav-number">2.5.</span> <span class="nav-text">偏差修正</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#动量momentum梯度下降"><span class="nav-number">2.6.</span> <span class="nav-text">动量(Momentum)梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rmsproproot-mean-square-prop算法"><span class="nav-number">2.7.</span> <span class="nav-text">RMSprop(Root Mean Square prop)算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adamadaptive-moment-estimation-优化算法"><span class="nav-number">2.8.</span> <span class="nav-text">Adam(Adaptive moment estimation) 优化算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#学习率衰减"><span class="nav-number">2.9.</span> <span class="nav-text">学习率衰减</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#局部最优解"><span class="nav-number">2.10.</span> <span class="nav-text">局部最优解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#week-3"><span class="nav-number">2.11.</span> <span class="nav-text">Week 3</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#batch-normalization"><span class="nav-number">3.</span> <span class="nav-text">Batch Normalization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#调参过程"><span class="nav-number">3.1.</span> <span class="nav-text">调参过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#选择合适尺度去选取超参数"><span class="nav-number">3.2.</span> <span class="nav-text">选择合适尺度去选取超参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#熊猫模型和鱼子酱模型"><span class="nav-number">3.3.</span> <span class="nav-text">熊猫模型和鱼子酱模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#批量归一化"><span class="nav-number">3.4.</span> <span class="nav-text">批量归一化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#将batch-norm运用到神经网络中"><span class="nav-number">3.5.</span> <span class="nav-text">将batch-norm运用到神经网络中</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么batch-normalization会有效"><span class="nav-number">3.6.</span> <span class="nav-text">为什么batch-normalization会有效</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对测试数据的batch-norm"><span class="nav-number">3.7.</span> <span class="nav-text">对测试数据的batch norm</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多分类"><span class="nav-number">4.</span> <span class="nav-text">多分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#softmax-regression"><span class="nav-number">4.1.</span> <span class="nav-text">softmax Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对softmax的理解"><span class="nav-number">4.2.</span> <span class="nav-text">对softmax的理解</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#深度学习框架的介绍"><span class="nav-number">5.</span> <span class="nav-text">深度学习框架的介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#tensorflow简介"><span class="nav-number">5.1.</span> <span class="nav-text">TensorFlow简介</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#损失计算"><span class="nav-number">5.1.1.</span> <span class="nav-text">损失计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#one_hot-encoding"><span class="nav-number">5.1.2.</span> <span class="nav-text">one_hot encoding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#实现tensorflow-model的步骤"><span class="nav-number">5.1.3.</span> <span class="nav-text">实现TensorFlow model的步骤</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#初始化参数的方法"><span class="nav-number">5.2.</span> <span class="nav-text">初始化参数的方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播的方法"><span class="nav-number">5.3.</span> <span class="nav-text">反向传播的方法</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jeffrey Pacino</p>
  <div class="site-description" itemprop="description">Humble to the dust and then out of the flowers</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">93</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">60</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jeffrey Pacino</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://muse.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.7.2
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
