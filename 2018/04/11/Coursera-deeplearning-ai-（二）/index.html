<!doctype html>



  


<html class="theme-next mist use-motion" lang="default">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
    
  
  <link href="//cdn.jsdelivr.net/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






  

<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.2/css/font-awesome.min.css" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css">


  <meta name="keywords" content="deeplearning,机器学习,深度学习,">








  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico?v=5.1.0">






<meta name="description" content="这篇博文主要讲的是关于deeplearning.ai的第二门课程的内容，《Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization》">
<meta name="keywords" content="deeplearning,机器学习,深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="Coursera-deeplearning-ai-（二）">
<meta property="og:url" content="http://drawwon.github.io/2018/04/11/Coursera-deeplearning-ai-（二）/index.html">
<meta property="og:site_name" content="WangZhao&#39;s Blog">
<meta property="og:description" content="这篇博文主要讲的是关于deeplearning.ai的第二门课程的内容，《Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization》">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/88387245.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/22424626.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/29396365.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/27719103.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/82915442.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/98592108.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/41548130.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/1978794.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-12/9225476.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/20190225112424.png">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-12/31073973.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-28/23613139.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-28/12421014.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-30/72186468.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-1/63473191.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-2/83045151.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-2/7255551.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-2/48630632.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-2/46166279.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-2/70350494.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/80735041.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/20607262.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/36826086.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/5744779.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/5343372.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/57297767.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/89577231.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/60312042.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/75203372.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/83926609.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-4/69914394.jpg">
<meta property="og:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-4/92497288.jpg">
<meta property="og:updated_time" content="2019-02-27T05:31:34.227Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Coursera-deeplearning-ai-（二）">
<meta name="twitter:description" content="这篇博文主要讲的是关于deeplearning.ai的第二门课程的内容，《Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization》">
<meta name="twitter:image" content="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/88387245.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: 'N00HWEGM76',
      apiKey: '916479ea342925b0950c99bd2b0401bb',
      indexName: 'drawon',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"我们没有找到任何搜索结果: ${query}","hits_stats":"找到约${hits}条结果（用时${time}ms）"}
    }
  };
</script>



  <link rel="canonical" href="http://drawwon.github.io/2018/04/11/Coursera-deeplearning-ai-（二）/">





  <title> Coursera-deeplearning-ai-（二） | WangZhao's Blog </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  














  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">WangZhao's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">It's not who you are underneath,it's what you do that defines you</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://drawwon.github.io/2018/04/11/Coursera-deeplearning-ai-（二）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeffrey Pacino">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WangZhao's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Coursera-deeplearning-ai-（二）
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-11T10:03:47+08:00">
                2018-04-11
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>这篇博文主要讲的是关于deeplearning.ai的第二门课程的内容，《Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization》</p>
<a id="more"></a>
<h2 id="Week-one"><a href="#Week-one" class="headerlink" title="Week one"></a>Week one</h2><h3 id="设置训练，验证，测试集"><a href="#设置训练，验证，测试集" class="headerlink" title="设置训练，验证，测试集"></a>设置训练，验证，测试集</h3><p>设置神经网络时，有很多的值需要你自己设置，比如隐藏层的数量，隐藏点的个数，学习率，激活函数的类型等等……</p>
<p>数据通常被分为三部分：训练集，hold-out交叉验证集（或者成为开发集dev），测试集。分布如下图所示：</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/88387245.jpg" alt></p>
<p>多年前，数据较少：70%的训练数据和30%的测试数据，或者60%训练数据，验证集和测试集各占20%</p>
<p>但现在数据越来越多，100w的总数据，验证集和测试集可能都只需要1w个就行了，剩下的98w数据都可以用于训练，比例为98/1/1</p>
<p>数据更多的时候，可能开发集和测试集所占的比例更小</p>
<h4 id="数据不平衡"><a href="#数据不平衡" class="headerlink" title="数据不平衡"></a>数据不平衡</h4><p>训练集，开发集，测试集的数据分布不同，比如图片识别中，两边的数据来源不同（一边是高清图片，一边是模糊图片），这时候只需要保证<strong>开发集和测试集在同一个分布</strong>即可。</p>
<h3 id="偏差方差"><a href="#偏差方差" class="headerlink" title="偏差方差"></a>偏差方差</h3><p>深度学习中有一个问题叫做“偏差-方差困境”，要在偏差和方差之间权衡</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/22424626.jpg" alt></p>
<h4 id="如何判断是高方差还是高偏差"><a href="#如何判断是高方差还是高偏差" class="headerlink" title="如何判断是高方差还是高偏差"></a>如何判断是高方差还是高偏差</h4><p>往往通过训练集误差和开发集误差的对比来进行判断：</p>
<ul>
<li>训练集误差小，开发集误差大，证明过拟合了，高方差</li>
<li>训练集误差大，开发集误差约等于训练集误差，证明欠拟合，高偏差</li>
<li>训练集误差大，开发集误差远大于训练集，证明高偏差且高方差，这是因为在某些数据上过拟合，而在大部分数据上欠拟合</li>
<li>训练集误差小，开发集误差也很小，这就是最理想的状态</li>
</ul>
<p>下面这个分类猫的例子比较直观解释了上面四种情况，注意，此时所谓的大小是因为我们设置的贝叶斯先验错误为0%，所以认为1%小，15%大。如果贝叶斯先验概率不是0%而是15%，那么15%的错误率也是很小的了。并且此时要求训练集和开发集的数据分布是相同的（如果一个是高质量数据，一个是低质量数据，那么两个本来错误率就不一样）。</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/29396365.jpg" alt></p>
<h3 id="机器学习的基本准则"><a href="#机器学习的基本准则" class="headerlink" title="机器学习的基本准则"></a>机器学习的基本准则</h3><p>训练好模型之后：</p>
<ul>
<li>首先询问，是否存在<strong>高偏差</strong>（在训练集上面的表现），如果存在，那么你可以尝试使用<strong>更大的网络</strong>（更多层和更多隐藏点），或者尝试训练<strong>更多的迭代次数</strong>。尝试多种方法，直到将偏差减小到一个可以接受的范围。</li>
<li>再看看是否有较高的<strong>方差</strong>（在开发集上面的表现），如果存在，那么比较好的办法就是<strong>增加训练数据</strong>，或者是<strong>正则化</strong></li>
</ul>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>如果发现<strong>过拟合</strong>，那么就是方差过大，首先应该尝试的方法就是正则化</p>
<p>以逻辑回归为例，为了最小化代价函数$J(W,b)=\frac{1}{m}\sum_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)})$，我们在后面加上一个W的范数，常用的范数为二范数，代价函数变为：</p>
<p>$J(W,b)=\frac{1}{m}\sum<em>{i=1}^{m}L(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}||w||^{2}</em>{2}$</p>
<p>其中的$\lambda$是正则化参数，$||w||^{2}<em>{2}$称为w的二范数，$||w||^{2}</em>{2}=\sum<em>{j=1}^{n</em>{x}}w_j^2=w^Tw$</p>
<p>为什么只对w正则化而忽略b呢，这是因为在过拟合的情况下，w的维度非常大，而b只有一个参数，影响相对于w来说可以忽略</p>
<p>偶尔也用一范数，但很少用，具体的逻辑回归的正则化方法如下</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/27719103.jpg" alt></p>
<h4 id="神经网络的正则化"><a href="#神经网络的正则化" class="headerlink" title="神经网络的正则化"></a>神经网络的正则化</h4><p>神经网路的正则化的方法和逻辑回归基本一样，只是w的二范数成了w矩阵的元素平方和，这个值被称为Frobenius norm（弗罗贝尼乌斯范数）</p>
<p>在反向传播的时候，反向传播的$dw^{[l]}$就成了原本的反向传播的值（下图中间绿色方框行，由代价函数J求导得到），加上$\frac{\lambda}{m}w^{[l]}$，w的更新公式就成了这样:</p>
<p>$w^{[l]}=(1-\frac{\alpha\lambda}{m})w^{[l]}-\alpha(原本的反向传播值)$</p>
<p>所以正则化之后，每次更新相当于只是在原本的w前面乘以一个略小于1的值$1-\frac{\alpha\lambda}{m}$，再减去原本的反向传播的值，因此神经网络的正则化又被称之为权重衰减</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/82915442.jpg" alt></p>
<h3 id="为什么正则化可以消除过拟合"><a href="#为什么正则化可以消除过拟合" class="headerlink" title="为什么正则化可以消除过拟合"></a>为什么正则化可以消除过拟合</h3><p>如图，如果过拟合，我们在加入正则化之后，如果把$\lambda$设置的非常大，那么为了是代价函数最小，w必须非常小，那么w的很多值就为0了，多层神经网络看上去就像是一个简单神经网路一样</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/98592108.jpg" alt></p>
<p>另一个直观解释是当你使用tanh之类的激活函数的时候，当$\lambda$非常大的时候，那么w非常小，因此z也非常小，经过激活函数变化之后的a也非常小，因此a值只能在0附近变化，这一段tanh函数基本相当于一个线性函数，也就是多层神经网络变化之后基本相当于在做线性变换，就变成一个接近线性变化的值</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/41548130.jpg" alt></p>
<h3 id="dropout-正则化"><a href="#dropout-正则化" class="headerlink" title="dropout 正则化"></a>dropout 正则化</h3><p>dropout正则化，也就是丢弃法正则化，也成为随机失活正则化。</p>
<p>对每个点进行抛硬币，50%的概率丢弃该点，得到一个丢弃一部分的神经网络，这个方法虽然听上去不可靠，但是实际表现却不错</p>
<p>随机失活正则化的实现方法：</p>
<p>假设有一个L=3的神经网络，先设置一个保留率keep-prob，随机产生一个3*n的矩阵，与keep-prob比较之后产生d3，让原本的w乘以这个d3，再除以一个keep-prob以消除引入随机失活的影响（因为你引入随机失活，相当于对某层的a乘以一个keep-prob，那么我要结果一样，就要除以一个keep-prob）</p>
<p>举个例子为什么要除以keep-prob，比如我们现在第三层有50个点，如果keep-prob为0.8的话，那么这层大概平均来说有10个点要失效，$z^{[4]}=w^{[4]}a^{[3]}+b^{[4]}$，那么此时的$a^{[3]}$的期望就变成了原来的80%，为了使得$z^{[4]}$的期望不变，我们就需要将$a^{[3]}$除以一个keep-prob来确保$z^{[4]}$期望不变。</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-11/1978794.jpg" alt></p>
<h3 id="随机失活正则化的理解"><a href="#随机失活正则化的理解" class="headerlink" title="随机失活正则化的理解"></a>随机失活正则化的理解</h3><p>直观解释：因为你不知道哪一个神经元可能被丢弃，所以你不能过分依赖某个神经元，因此权重就不得不分散</p>
<p>在真正使用的时候，如果你担心某层容易过拟合，那么就把这一层的留存率设置的低一些；如果确认不会过拟合，那就把留存率设置接近1，比如在输入层这里留存率就应该是1</p>
<h3 id="其它正则化方法"><a href="#其它正则化方法" class="headerlink" title="其它正则化方法"></a>其它正则化方法</h3><p>在图片处理的时候，如果你没有更多的数据，比如处理猫之类的：你可以将图片进行水平翻转，或者放大旋转之类的，处理数字的时候：可以扭曲加旋转</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-12/9225476.jpg" alt></p>
<p>另一种方法叫做<strong>早终止方法</strong>（early stopping）</p>
<p>画出训练集的代价函数和开发集的代价函数，选择两者都还比较小的值</p>
<h3 id="归一化（normalization）"><a href="#归一化（normalization）" class="headerlink" title="归一化（normalization）"></a>归一化（normalization）</h3><p>归一化可以加速训练过程</p>
<p>归一化的过程：<strong>减去均值（$x-\mu$），将方差归一化$(x-\mu)/\sigma$</strong></p>
<p>归一化过程中一定要注意，对训练集和测试集都需要归一化</p>
<h3 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h3><p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/20190225112424.png" alt></p>
<p>由于：</p>
<script type="math/tex; mode=display">a^{[l]}=\sigma(w^{[l]}a^{[l-1]}+b^{[l]}) \tag{1}</script><script type="math/tex; mode=display">a^{[l-1]}=\sigma(w^{[l-1]}a^{[l-2]}+b^{[l-1]}) \tag{2}</script><script type="math/tex; mode=display">a^{[l-2]}=\sigma(w^{[l-2]}a^{[l-3]}+b^{[l-2]}) \tag{3}</script><p>求导可得</p>
<p>$dz^{[l]}=da^{[l]} * {g^{[l]}}’(z^{[l]}) \tag{4}​$</p>
<p>$dw^{[l]}=dz^{[l]}*a^{[l-1]}\tag{5} $</p>
<p>$db^{[l]}=dz^{[l]}\tag{6}​$</p>
<p>接着求前一层：</p>
<p>由上面公式(1)可以得到：</p>
<p>$da^{[l-1]}=dz^{[l]}*w^{[l]}\tag{7}​$</p>
<p>由公式(2)得到：</p>
<p>$dz^{[l-1]}=da^{[l-1]} * {g^{[l-1]}}’(z^{[l-1]}) \tag{8}​$</p>
<p>$dw^{[l-1]}=dz^{[l-1]}*a^{[l-2]} \tag{9}​$</p>
<p>结合(7),(8),(9)得到：</p>
<p>$dw^{[l-1]}=dz^{[l]}<em>w^{[l]}</em> {g^{[l-1]}}’(z^{[l-1]}) \tag{10}$</p>
<p>继续对(3)进行求导：</p>
<p>$dz^{[l-2]}=da^{[l-2]} * {g^{[l-2]}}’(z^{[l-2]}) \tag{11}​$</p>
<p>$dw^{[l-2]}=dz^{[l-2]}*a^{[l-3]} \tag{12}$</p>
<p>由公式(2)得到：</p>
<p>$da^{[l-2]}=dz^{[l-1]}*w^{[l-1]}\tag{13}​$</p>
<p>结合(11),(12),(13)得到：</p>
<p>$dw^{[l-2]}=dz^{[l-1]}<em>w^{[l-1]}</em>{g^{[l-2]}}’(z^{[l-2]})*a^{[l-3]}​$</p>
<p>再结合(4),(7),(8)可得</p>
<p>$dw^{[l-2]}=da^{[l]}<em>w^{[l]}</em>w^{[l-1]}<em>{g^{[l]}}’(z^{[l]})</em>{g^{[l-1]}}’(z^{[l-1]}) <em>{g^{[l-2]}}’(z^{[l-2]})</em>a^{[l-3]}$</p>
<p>比如你的激活函数是g(z)=z，损失函数是交叉熵函数，$da=a-y​$然后$dw^{[1]}=w^{[l]}\times w^{[l-1]}\times…\times w^{[2]}\times w^{[1]}\times X​$，只要所有w都是对角矩阵，他的某一项大于1，则出现梯度爆炸，求出的梯度非常大，或者是梯度消失，求出的梯度基本为0</p>
<h3 id="权重初始化和深度网络"><a href="#权重初始化和深度网络" class="headerlink" title="权重初始化和深度网络"></a>权重初始化和深度网络</h3><p>特殊地初始化可以部分解决梯度爆炸和梯度消失的问题</p>
<p>在使用Relu激活函数的时候：</p>
<p>$W^{[L]}=np.random.randn(shape) * np.sqrt(1/n)​$</p>
<h3 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h3><p>根据导数的定义，对代价函数进行求导：</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-12/31073973.jpg" alt></p>
<p>检查：两个导数之间的欧式距离/两个导数的2范数之和，如果基本等于$\epsilon$的话，那就说明正确了，如果大于$\epsilon$很多的话，就说明错了</p>
<h3 id="梯度下降的实现"><a href="#梯度下降的实现" class="headerlink" title="梯度下降的实现"></a>梯度下降的实现</h3><ul>
<li>只在调试的时候用提督检验，在训练的时候不要用</li>
<li>如果算法梯度检验失败，检查每一个dw，db来找到程序的bug</li>
<li>记得正则化</li>
<li>在没有dropout的时候先进行梯度检验，发现算法没有问题再使用dropout</li>
<li>随机初始化可以先运行一下梯度检验</li>
</ul>
<h3 id="合适的初始化方法"><a href="#合适的初始化方法" class="headerlink" title="合适的初始化方法"></a>合适的初始化方法</h3><p>He初始化方法（<a href="https://arxiv.org/abs/1502.01852" target="_blank" rel="noopener">He et al., 2015</a>），在激活函数是Relu的时候非常有效，具体做法是$W^{[l]}=\rm{np.random.randn}(layer_dimension[l],layer_dimension[l-1])*\rm{np.sqrt}(2./layer_dimension[l-1])$</p>
<h2 id="第二周"><a href="#第二周" class="headerlink" title="第二周"></a>第二周</h2><h3 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h3><p>向量化可以高效计算m个example，但是当example非常多的时候，计算起来也是非常的慢的，比如你现在有500w个example，拿计算起来就是非常慢的</p>
<p>为了加快计算的速度，提出了mini-batch gradient descent，也就是批量梯度下降，将数据分成一个个的小batch，然后进行前向传播，反向传播，参数更新等步骤，这样计算速度会快上很多</p>
<p>比如现在有500w条数据，将每1000条数据凑成一个batch，用<code>{}</code>来表示第多少个batch，现在分成了$X^1$到$X^5000$共5000个batch，每个batch的维度是$(n_x,1000)$</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-28/23613139.jpg" alt></p>
<p>Y以同样的方法被分成5000份，每个$Y^$的维度是(1，1000)</p>
<p>到目前为止，我们一共用过三种括号，分别是小括号，中括号，和大括号</p>
<ul>
<li>小括号：$x^{(i)}$，表示第i个训练实例</li>
<li>中括号：$Z^{[L]}$表示第L层</li>
<li>大括号：$X^$,$Y^$表示第t个batch</li>
</ul>
<p>分成batch之后的步骤和之前的神经网络的构建步骤一样，只是多了一重循环batch的for</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-28/12421014.jpg" alt></p>
<h3 id="理解mini-batch梯度下降"><a href="#理解mini-batch梯度下降" class="headerlink" title="理解mini-batch梯度下降"></a>理解mini-batch梯度下降</h3><p>批量梯度下降的损失函数往往一直下降，但是mini-batch梯度下降存在噪声，但是整体趋势是下降的</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-4-30/72186468.jpg" alt></p>
<p>两种极端情况：</p>
<ol>
<li>如果mini-batch的size=m，那么这就是梯度下降，梯度下降的好处是每一步迭代都是往最优值的方向去靠近，但是数据量很大的时候，批量梯度下降就会非常的慢，这种情况又被称为批梯度下降</li>
<li>如果mini-batch的size=1，那么这种情况就是每次输入一个example，这样每次迭代的方向可能是乱的，最终的结果可能在最优值附近徘徊，这种情况又被称为随机梯度下降</li>
<li>只有mini-batch值合适的时候，才能既用到向量化的加速运算，又能得到一个最优值</li>
</ol>
<p>一般认为：</p>
<p>在m&lt;=2000时，认为数据量足够下，可以使用批量梯度下降</p>
<p>在m&gt;2000时，通常使用的mini-batch的size为64, 128, 256, 512，用2的倍数是因为内存读取的方式是通过2的倍数来读取的，这样能够加快运算</p>
<h3 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h3><p>如图，是一大堆温度数据，我们为了对温度数据做个平均，用v0=0,$v<em>1=0.9v_0+0.1\theta_1$，一直到$v_t=0.9v</em>{t-1}+0.1\theta_t$进行指数加权平均</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-1/63473191.jpg" alt></p>
<p>这种指数加权平均的效果的$v<em>{t}$就大致等同于对$\frac{1}{1-\beta}$天的数据进行平均，其中$\beta$是$v_t=\beta v</em>{t-1}+(1-\beta)\theta_t$这个公式中的系数</p>
<p>比如，当$\beta=0.9$时，这就相当于对前10天的数据进行平均；当的$\beta=0.98$时，这就相当于对前50天的数据进行平均；当的$\beta=0.5$时，这就相当于对前2天的数据进行平均</p>
<p>更大的$\beta$意味着更平滑的曲线，但是对数据的延迟性也更大</p>
<h3 id="指数加权平均的理解"><a href="#指数加权平均的理解" class="headerlink" title="指数加权平均的理解"></a>指数加权平均的理解</h3><p>通用的迭代公式：$v<em>t=\beta v</em>{t-1}+(1-\beta)\theta_t $</p>
<p>我们来举个例子，假如$\beta=0.9$</p>
<p>那么</p>
<p>$v<em>{100}=0.9 v</em>{99}+0.1\theta_{100}$</p>
<p>$v<em>{99}=0.9 v</em>{98}+0.1\theta_{99}$</p>
<p>$v<em>{98}=0.9 v</em>{97}+0.1\theta_{98}$</p>
<p>将$v<em>{100}=0.9 v</em>{99}+0.1\theta_{100}$展开可以得到</p>
<p>$v<em>{100}=0.9 v</em>{99}+0.1\theta<em>{100}=0.1\theta</em>{100}+0.9(0.1\theta<em>{99}+0.9 v</em>{98})=0.1\theta<em>{100}+0.9*0.1\theta</em>{99}+0.9^2(0.9 v<em>{97}+0.1\theta</em>{98})…$ </p>
<p>这个过程与我们平时的平均数有类似的地方，因为我们平时求解的平均数是在每个$\theta$前面的系数相等，都是1/n，在指数加权平均的时候，将靠的近的系数放大，靠的远的系数变小，以指数形式衰减</p>
<p>这样下去，要使得v的加和的那一项足够小， 也就是$0.1*0.9^{t}$足够小的情况下，$0.9^{10}=1/e$，就认为是10天的平均</p>
<p><strong>指数加权平均的好处：</strong> </p>
<p>我们可以看到指数加权平均的求解过程实际上是一个递推的过程，那么这样就会有一个非常大的好处，每当我要求从0到某一时刻（n）的平均值的时候，我并不需要像普通求解平均值的作为，保留所有的时刻值，类和然后除以n。</p>
<p>而是只需要保留0-(n-1)时刻的平均值和n时刻的温度值即可。也就是每次只需要保留常数值，然后进行运算即可，这对于深度学习中的海量数据来说，是一个很好的减少内存和空间的做法。</p>
<h3 id="偏差修正"><a href="#偏差修正" class="headerlink" title="偏差修正"></a>偏差修正</h3><p>因为$v_0=0$，而$v_1=0.98v_0+0.02\theta_1$，因为$v_0=0$，所以$v_1=0.02\theta_1$；$v_2=0.98v_1+0.02\theta_2$，$v_2=0.0196\theta_1+0.02\theta_2$</p>
<p>由于上面两个等式展现的原因，这些v的值在初始阶段都很小，为了使这些初始阶段的值可以作为平均，我们用$v_t=\frac{v_t}{1-\beta^t}$来进行偏差修正，如下图</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-2/83045151.jpg" alt></p>
<h3 id="动量-Momentum-梯度下降"><a href="#动量-Momentum-梯度下降" class="headerlink" title="动量(Momentum)梯度下降"></a>动量(Momentum)梯度下降</h3><p>动量梯度下降比普通的梯度下降更快，其主要思想是：计算梯度的指数加权平均，使用这个梯度来更新权重</p>
<p>实现的方式如下，$\beta$参数最常用的值就是0.9：</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-2/7255551.jpg" alt></p>
<p>进行动量梯度下降之后，纵轴上的偏差被减小了，得到如下图红线的效果</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-2/48630632.jpg" alt></p>
<h3 id="RMSprop-Root-Mean-Square-prop-算法"><a href="#RMSprop-Root-Mean-Square-prop-算法" class="headerlink" title="RMSprop(Root Mean Square prop)算法"></a>RMSprop(Root Mean Square prop)算法</h3><p>实现的方法和momentum类似，但是公式变成了</p>
<p>$S<em>{dw}=\beta_2S</em>{dw}+(1-\beta_2)dw^{2}$</p>
<p>$S<em>{db}=\beta_2S</em>{db}+(1-\beta_2)db^{2}$</p>
<p>而迭代公式变成了</p>
<p>$w:=w-\alpha\frac{dw}{\sqrt{S_{dw}}+\epsilon}$</p>
<p>$b:=b-\alpha\frac{dw}{\sqrt{S_{db}}+\epsilon}$</p>
<p>加一个$\epsilon$是为了不出现除以0的情况</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-2/46166279.jpg" alt></p>
<h3 id="Adam-Adaptive-moment-estimation-优化算法"><a href="#Adam-Adaptive-moment-estimation-优化算法" class="headerlink" title="Adam(Adaptive moment estimation) 优化算法"></a>Adam(Adaptive moment estimation) 优化算法</h3><p>Adam(Adaptive moment estimation)的意思是：适应性矩优化，这里的矩指的是一阶矩，二阶矩那个矩。</p>
<p>Adam就是将momentum和RMSprop结合起来</p>
<p>实现方法如下图，注意这里的参数都需要修正偏差：</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-2/70350494.jpg" alt></p>
<p>里面的超参，一般来说momentum的超参$\beta_1=0.9$，RMSprop的超参$\beta_2=0.999$，$\epsilon=10^{-8}$，学习率$\alpha$ 是需要去调整的参数，Adam的公式如下，将w换成b则得到b的更新公式</p>
<script type="math/tex; mode=display">
\begin{cases}
v_{dW^{[l]}} = \beta_1 v_{dW^{[l]}} + (1 - \beta_1) \frac{\partial \mathcal{J} }{ \partial W^{[l]} } \\

v^{corrected}_{dW^{[l]}} = \frac{v_{dW^{[l]}}}{1 - (\beta_1)^t} \\
s_{dW^{[l]}} = \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial W^{[l]} })^2 \\
s^{corrected}_{dW^{[l]}} = \frac{s_{dW^{[l]}}}{1 - (\beta_2)^t} \\
W^{[l]} = W^{[l]} - \alpha \frac{v^{corrected}_{dW^{[l]}}}{\sqrt{s^{corrected}_{dW^{[l]}}} + \varepsilon}
\end{cases}</script><h3 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h3><p>我们用下面的公式来衰减学习率$\alpha$：</p>
<p>$\alpha=\frac{1}{1+decay_rate\times epoch_num}\alpha_0$</p>
<p>decay_rate是这里的下降率，epoch_num是迭代的次数</p>
<h3 id="局部最优解"><a href="#局部最优解" class="headerlink" title="局部最优解"></a>局部最优解</h3><p>在二维图像中，很容易产生局部最优解，但是在高维的时候，你要找到一个这个点在所有维度上梯度都为0，这是非常困难的，我们称这种有部分维度梯度为0的点为鞍点，因为图形的形状就好像马鞍一样</p>
<h3 id="Week-3"><a href="#Week-3" class="headerlink" title="Week 3"></a>Week 3</h3><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><h3 id="调参过程"><a href="#调参过程" class="headerlink" title="调参过程"></a>调参过程</h3><p>神经网络有很多的超参，调整超参有利于改进神经网络的性能</p>
<p>参数有很多，包括：学习率$\alpha$，momentum当中的$\beta$，Adam优化中的$\beta_1,\beta_2,\epsilon$，网络层数，隐藏单元，学习率衰减方式，mini-batch的size</p>
<p>一般来说需要调整的重要程度排序为：</p>
<p>$\alpha&gt;\rm{momentum当中的}\beta=mini-batch\ size=隐藏单元数量&gt;网络层数&gt;学习率衰减参数&gt;&gt;Adam（Adam一般不调参，用默认参数\beta_1=0.9,\beta_2=0.999,\epsilon=10^{-8}）$</p>
<p>但这并不是一个死板的规定，可能有其他的规则</p>
<p>早期调参的时候，通常是启发式搜索，然后给定最优的参数；参数很多的时候，建议随机选择点，进行尝试，如下图右边</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/80735041.jpg" alt></p>
<p>当你能确定更小的范围的时候，就可以在这个范围内进行更加密集的搜索，直到找到你能接受的最优参数</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/20607262.jpg" alt></p>
<h3 id="选择合适尺度去选取超参数"><a href="#选择合适尺度去选取超参数" class="headerlink" title="选择合适尺度去选取超参数"></a>选择合适尺度去选取超参数</h3><p>很多超参数是不能在某个范围内均匀取样的，比如考虑学习率$\alpha$，让$\alpha$从0.0001到1取值，肯定要求在0.0001到0.001之前取多点，而0.1-1之间要比较少，所以我们此时用到对数的取法，也就是从10e-4取到10e0，那我们就只需要去一个-4到0的随机数，用a = -4 <em> np.random.randn(), alpha=10*</em>a</p>
<p>还有比如momentum当中的$\beta$参数，如果让$\beta$从0.9取到0.999，在靠近0.999的时候，稍微改变一点点都会让平均值的范围变化很大，因此在后面我们要取的密集一些，我们考虑$1-\beta$，$\beta$从0.9到0.999，那么$1-\beta$就从0.1到0.001，取一个从-3到-1的随机数，再用10的指数来代替$1-\beta$，那么$\beta=1-10^t$</p>
<h3 id="熊猫模型和鱼子酱模型"><a href="#熊猫模型和鱼子酱模型" class="headerlink" title="熊猫模型和鱼子酱模型"></a>熊猫模型和鱼子酱模型</h3><p>熊猫模型：关注你的模型，就如同熊猫产子一般，一次调整一点</p>
<p>鱼子酱模型：一次同时开始多个模型的训练，如同鱼类产子一般</p>
<p>计算资源足够的时候，就用鱼子酱模型，否则用熊猫模型</p>
<p>这两个名称只是为了好记忆，并没有特别的意思</p>
<h3 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h3><p>在之前的归一化当中，我们只是对第一步的输入进行了归一化，但是其实每一层神经网络的输入应该都有归一化，在归一化z和a这两种选择中，业界都默认归一化z</p>
<p>对z的归一化过程如下：</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/36826086.jpg" alt></p>
<p>红框部分就是归一化的过程，对于每一个z(i)，计算均值$\mu$，方差$\sigma^2$，然后用$z^{(i)}<em>{norm}=\frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\epsilon}}$，这里加一个$\epsilon$的原因是为了避免除以0的情况发生，然后用$\tilde{z}^{(i)}=\gamma z^{(i)}</em>{norm}+\beta$，这个$\gamma$和$\beta$是可以从模型当中学习出来的参数。</p>
<p>为什么要用$\gamma$和$\beta$这两个参数呢，是因为比如你中间某一层的激活函数是sigmoid函数，如果你让你的z均值为0，方差为1，那么z的变化范围就很靠近0，这是sigmoid函数基本就成了线性函数，为了利用好sigmoid的非线性，所以对中间的z的归一化稍有不同</p>
<h3 id="将batch-norm运用到神经网络中"><a href="#将batch-norm运用到神经网络中" class="headerlink" title="将batch-norm运用到神经网络中"></a>将batch-norm运用到神经网络中</h3><p>假设我们有一个如下图所示的三层神经网络，那么我们将x输入，通过w[1]和b[1]，得到z[1]，对z1进行batch-norm，通过$\gamma^{[1]}$和$\beta^{[1]}$得到$\tilde{z}^{[1]}$，然后将$\tilde{z}^{[1]}$通过g[1]得到a[1]，同理得到z[2]，$\tilde{z}^{[2]}$，a[2]</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/5744779.jpg" alt></p>
<p>此时的参数就有了w[1],b[1],w[2],b[2]，$\gamma^{[1]}$,$\beta^{[1]}$,$\gamma^{[2]}$,$\beta^{[2]}$,在TensorFlow中我们可以直接一行语句实现batch-normalization,<code>tf.nn.batch-normalization</code></p>
<p>那如何将batch-normalization用到mini-batch-normalization中呢</p>
<p>如下图，每次用一个mini-batch，对其进行batch-normalization。</p>
<p>值得注意的是，因为$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$，而$z_{norm}=\frac{z-\mu}{\sqrt{\sigma^2+\epsilon}}$，每次归一化的时候减去了均值，所以加的$b^{[l]}$会被减掉，因此b这个参数在mini-batch-normalization时可以忽略</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/5343372.jpg" alt></p>
<p>实现的具体方法，对于每一次mini-batch t，计算对于$X^{[t]}$的前向传播，对每个隐藏层使用BN（batch-normalization）方法，然后反向传播去更新W,$\beta$,$\gamma$三个参数（b被减掉因此忽略），当然这里更新的方式可以是momentum，RMSprop或者Adam</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/57297767.jpg" alt></p>
<h3 id="为什么batch-normalization会有效"><a href="#为什么batch-normalization会有效" class="headerlink" title="为什么batch-normalization会有效"></a>为什么batch-normalization会有效</h3><p>首先，normalization会使得所有的x的值在同一个量级上面，这样能够加速迭代</p>
<p>协变量转换（covariate shift）是指在数据x变化之后，原来的网络不适用于分类新的数据的情况，如果我们使用了batch-normalization方法，前面层的变化对后面层的影响就降低了，因为被平均了，所以BN会使得系统优化的结果更好</p>
<p>同时，这还起到了一点点正则化的作用，因为每个mini-batch在计算的时候都被平均了，所以整个网络对于数据的适应性就没有那么强了</p>
<h3 id="对测试数据的batch-norm"><a href="#对测试数据的batch-norm" class="headerlink" title="对测试数据的batch norm"></a>对测试数据的batch norm</h3><p>在训练阶段，我们每次可以用一次批量的值计算均值和方差，但是在测试阶段，我们每次输入的只有一个值，这时候我们进行batch norm的均值和方差从哪里来呢？</p>
<p>解决办法就是，记录下训练数据的均值和方差，然后对各个mini-batch norm的均值和方差做指数权重平均，在测试阶段使用</p>
<h2 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h2><h3 id="softmax-Regression"><a href="#softmax-Regression" class="headerlink" title="softmax Regression"></a>softmax Regression</h3><p>我们之前接触的问题都是二分类，当我们要进行多分类的时候，就要用到一个特殊的激活函数，叫做softmax</p>
<p>假设我们要分类的类别数C=4，标签为0,1,2,3，那么在最后一层，我们要输出一个4*1的输出层，每一个输出点代表分到该类的概率</p>
<p>举个例子，我们得到了最后一层的输入为z[L] = [5,2,-1,3]，我们用指数函数对其变换，$t = [e^5,e^2,e^{-1},e^3]$，计算比例得到$a^{[L]}$，如下图所示</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/89577231.jpg" alt></p>
<h3 id="对softmax的理解"><a href="#对softmax的理解" class="headerlink" title="对softmax的理解"></a>对softmax的理解</h3><p>softmax是一个$\frac{e^{z_j}}{\sum_ke^{z_k}}$形式的激活函数，当分类的类别C=2的时候，softmax就是logistics函数</p>
<p>softmax的loss一般取为：$L(\hat{y},y)=-\sum_{j=1}^Cy_j\log \hat{y}_j$</p>
<p>真实的y和$\hat y$的形式如下：</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/60312042.jpg" alt></p>
<p>真实值只有真的那个地方为1，别的地方为0，$\hat y$是C个概率，代表分到每一类的概率</p>
<p>因为y一般有很多个需要分类的样本，所以真实的y和$\hat y$如下，其中的4是此时分为4类</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/75203372.jpg" alt></p>
<p>反向传播中，softmax的导数的求法稍微复杂一点，过程如下：</p>
<p>首先求$\partial J/\partial a$，虽然这里有个累加，但是其实只有真实的那类$y_j=1$，别的都是0，所以求和号可以去掉，变成$J=y_j\log \hat{y}_j$，对$\hat y$求偏导可以得到，$\partial J/\partial a=-1/\hat{y}_j$</p>
<p>接下来求softmax的导数，也就是$\hat{y}_j$对所有的$z_i$求导数，分为i=j和i!=j的情况来求</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-3/83926609.jpg" alt></p>
<p>这样，$\partial J/\partial z$的值就可以通过链式法则得到</p>
<p>当i=j时，$\partial J/\partial z=a_j-1$</p>
<p>当i!=j时，$\partial J/\partial z=-a_i$</p>
<p>在使用深度学习框架的时候，比如TensorFlow和caffe，我们只需要规划好前向传播的过程，反向传播的过程框架会自动帮你完成</p>
<h2 id="深度学习框架的介绍"><a href="#深度学习框架的介绍" class="headerlink" title="深度学习框架的介绍"></a>深度学习框架的介绍</h2><p>目前主流的深度学习框架和选择标准如下：</p>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-4/69914394.jpg" alt></p>
<h3 id="TensorFlow简介"><a href="#TensorFlow简介" class="headerlink" title="TensorFlow简介"></a>TensorFlow简介</h3><p>引入TensorFlow，通过<code>import tensorflow as tf</code></p>
<p>w设置为tf当中的变量，用<code>tf.Variable(initial_value=0,dtype=tf.float32)</code>表示</p>
<p>x是输入值，一开始不知道是多少，只表示dtype和shape，用<code>tf.placeholder(dtype=tf.float32,shape=[3,1])</code>表示</p>
<p>表示cost函数，因为tf已经重载了加减乘除的形式，所以可以直接用加减乘除表示，也可以用<code>tf.add</code>之类的表示，矩阵乘法的表示是<code>tf.matmul()</code></p>
<p>之后表示train的方法和目标：我们这里用梯度下降，最小化cost<code>train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)</code>，如果要用别的优化方法，只需要将<code>GradientDescentOptimizer</code>替换为别的函数就好了，括号里面的参数是learning-rate</p>
<p>然后初始化变量值，<code>init = tf.global_variables_initializer()</code></p>
<p>定义一个session，用session来run一下init，再run一下w，看看w的值，最后迭代run(train)</p>
<p>也可以用如下形式定义session</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">	session.run(init)</span><br><span class="line">    session.run(w)</span><br></pre></td></tr></table></figure>
<p>placeholder的值可以用feed_dict传入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">x = tf.placeholder(tf.int64, name = <span class="string">'x'</span>)</span><br><span class="line">print(sess.run(<span class="number">2</span> * x, feed_dict = &#123;x: <span class="number">3</span>&#125;))</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<p><img src="https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/pics/18-5-4/92497288.jpg" alt></p>
<p>写TensorFlow的代码过程大致如下：</p>
<ol>
<li>建立未执行的tensor变量</li>
<li>写tensor之间的运算</li>
<li>初始化tensor</li>
<li>建立session</li>
<li>运行session，将会运行你简历里的运算</li>
</ol>
<p>所有的运算都要run之后才能执行，如果你直接print运算的话，只会得到一个tensor，也就是计算图</p>
<p>因此，请注意初始化变量，建立session并run operation</p>
<h4 id="损失计算"><a href="#损失计算" class="headerlink" title="损失计算"></a>损失计算</h4><p>计算形如：</p>
<script type="math/tex; mode=display">J = - \frac{1}{m}  \sum_{i = 1}^m  \large ( \small y^{(i)} \log a^{ [2] (i)} + (1-y^{(i)})\log (1-a^{ [2] (i)} )\large )\small</script><p>这样的损失的时候，可以使用tf内置的<code>tf.nn.sigmoid_cross_entropy_with_logits</code>函数实现</p>
<h4 id="one-hot-encoding"><a href="#one-hot-encoding" class="headerlink" title="one_hot encoding"></a>one_hot encoding</h4><p>one_hot：只有一个值为1，别的值都为0的vector，用<code>tf.one_hot</code>实现，参数<code>indices</code>表示需要转换的向量, <code>depth</code>表示一共多少个类， <code>on_value=None</code>表示符合类的值为多少, <code>off_value=None</code>表示不符合类的值是多少, <code>axis</code>为0表示每个indices放一行，-1表示每个indices放一列</p>
<h4 id="实现TensorFlow-model的步骤"><a href="#实现TensorFlow-model的步骤" class="headerlink" title="实现TensorFlow model的步骤"></a>实现TensorFlow model的步骤</h4><ol>
<li>建立一个计算图</li>
<li>run这个计算图</li>
</ol>
<h3 id="初始化参数的方法"><a href="#初始化参数的方法" class="headerlink" title="初始化参数的方法"></a>初始化参数的方法</h3><p>W用Xavier初始化，b用zero初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W1 = tf.get_variable(<span class="string">"W1"</span>, [<span class="number">25</span>,<span class="number">12288</span>], initializer = tf.contrib.layers.xavier_initializer()）</span><br><span class="line">b1 = tf.get_variable(<span class="string">"b1"</span>, [<span class="number">25</span>,<span class="number">1</span>], initializer = tf.zeros_initializer())</span><br></pre></td></tr></table></figure>
<h3 id="反向传播的方法"><a href="#反向传播的方法" class="headerlink" title="反向传播的方法"></a>反向传播的方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#For instance, for gradient descent the optimizer would be:</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)</span><br><span class="line"><span class="comment">#To make the optimization you would do:</span></span><br><span class="line">_ , c = sess.run([optimizer, cost], feed_dict=&#123;X: minibatch_X, Y: minibatch_Y&#125;)</span><br></pre></td></tr></table></figure>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/deeplearning/" rel="tag"># deeplearning</a>
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/04/10/c++ primer-基础语法/" rel="next" title="c++ primer 基础语法">
                <i class="fa fa-chevron-left"></i> c++ primer 基础语法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/04/16/selenium-爬取ajax动态网页/" rel="prev" title="selenium 爬取ajax动态网页">
                selenium 爬取ajax动态网页 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
  <a class="jiathis_button_tsina"></a>
  <a class="jiathis_button_tqq"></a>
  <a class="jiathis_button_weixin"></a>
  <a class="jiathis_button_cqq"></a>
  <a class="jiathis_button_douban"></a>
  <a class="jiathis_button_renren"></a>
  <a class="jiathis_button_qzone"></a>
  <a class="jiathis_button_kaixin001"></a>
  <a class="jiathis_button_copy"></a>
  <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank"></a>
  <a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript">
  var jiathis_config={
    hideMore:false
  }
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END -->

      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="Jeffrey Pacino">
          <p class="site-author-name" itemprop="name">Jeffrey Pacino</p>
           
              <p class="site-description motion-element" itemprop="description">Humble to the dust and then out of the flowers</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">93</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">44</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">60</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/drawwon" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://wpa.qq.com/msgrd?v=3&uin=847707695&site=qq&menu=yes" target="_blank" title="QQ">
                  
                    <i class="fa fa-fw fa-qq"></i>
                  
                  QQ
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="mailto:jeffrey.pacino@gmail.com" target="_blank" title="email">
                  
                    <i class="fa fa-fw fa-paper-plane"></i>
                  
                  email
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-one"><span class="nav-number">1.</span> <span class="nav-text">Week one</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#设置训练，验证，测试集"><span class="nav-number">1.1.</span> <span class="nav-text">设置训练，验证，测试集</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#数据不平衡"><span class="nav-number">1.1.1.</span> <span class="nav-text">数据不平衡</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#偏差方差"><span class="nav-number">1.2.</span> <span class="nav-text">偏差方差</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#如何判断是高方差还是高偏差"><span class="nav-number">1.2.1.</span> <span class="nav-text">如何判断是高方差还是高偏差</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#机器学习的基本准则"><span class="nav-number">1.3.</span> <span class="nav-text">机器学习的基本准则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正则化"><span class="nav-number">1.4.</span> <span class="nav-text">正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#神经网络的正则化"><span class="nav-number">1.4.1.</span> <span class="nav-text">神经网络的正则化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么正则化可以消除过拟合"><span class="nav-number">1.5.</span> <span class="nav-text">为什么正则化可以消除过拟合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dropout-正则化"><span class="nav-number">1.6.</span> <span class="nav-text">dropout 正则化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机失活正则化的理解"><span class="nav-number">1.7.</span> <span class="nav-text">随机失活正则化的理解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#其它正则化方法"><span class="nav-number">1.8.</span> <span class="nav-text">其它正则化方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#归一化（normalization）"><span class="nav-number">1.9.</span> <span class="nav-text">归一化（normalization）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度消失和梯度爆炸"><span class="nav-number">1.10.</span> <span class="nav-text">梯度消失和梯度爆炸</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#权重初始化和深度网络"><span class="nav-number">1.11.</span> <span class="nav-text">权重初始化和深度网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度检验"><span class="nav-number">1.12.</span> <span class="nav-text">梯度检验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降的实现"><span class="nav-number">1.13.</span> <span class="nav-text">梯度下降的实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#合适的初始化方法"><span class="nav-number">1.14.</span> <span class="nav-text">合适的初始化方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第二周"><span class="nav-number">2.</span> <span class="nav-text">第二周</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#优化算法"><span class="nav-number">2.1.</span> <span class="nav-text">优化算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#理解mini-batch梯度下降"><span class="nav-number">2.2.</span> <span class="nav-text">理解mini-batch梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#指数加权平均"><span class="nav-number">2.3.</span> <span class="nav-text">指数加权平均</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#指数加权平均的理解"><span class="nav-number">2.4.</span> <span class="nav-text">指数加权平均的理解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#偏差修正"><span class="nav-number">2.5.</span> <span class="nav-text">偏差修正</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#动量-Momentum-梯度下降"><span class="nav-number">2.6.</span> <span class="nav-text">动量(Momentum)梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RMSprop-Root-Mean-Square-prop-算法"><span class="nav-number">2.7.</span> <span class="nav-text">RMSprop(Root Mean Square prop)算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adam-Adaptive-moment-estimation-优化算法"><span class="nav-number">2.8.</span> <span class="nav-text">Adam(Adaptive moment estimation) 优化算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#学习率衰减"><span class="nav-number">2.9.</span> <span class="nav-text">学习率衰减</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#局部最优解"><span class="nav-number">2.10.</span> <span class="nav-text">局部最优解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Week-3"><span class="nav-number">2.11.</span> <span class="nav-text">Week 3</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-Normalization"><span class="nav-number">3.</span> <span class="nav-text">Batch Normalization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#调参过程"><span class="nav-number">3.1.</span> <span class="nav-text">调参过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#选择合适尺度去选取超参数"><span class="nav-number">3.2.</span> <span class="nav-text">选择合适尺度去选取超参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#熊猫模型和鱼子酱模型"><span class="nav-number">3.3.</span> <span class="nav-text">熊猫模型和鱼子酱模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#批量归一化"><span class="nav-number">3.4.</span> <span class="nav-text">批量归一化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#将batch-norm运用到神经网络中"><span class="nav-number">3.5.</span> <span class="nav-text">将batch-norm运用到神经网络中</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么batch-normalization会有效"><span class="nav-number">3.6.</span> <span class="nav-text">为什么batch-normalization会有效</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对测试数据的batch-norm"><span class="nav-number">3.7.</span> <span class="nav-text">对测试数据的batch norm</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多分类"><span class="nav-number">4.</span> <span class="nav-text">多分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#softmax-Regression"><span class="nav-number">4.1.</span> <span class="nav-text">softmax Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对softmax的理解"><span class="nav-number">4.2.</span> <span class="nav-text">对softmax的理解</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#深度学习框架的介绍"><span class="nav-number">5.</span> <span class="nav-text">深度学习框架的介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TensorFlow简介"><span class="nav-number">5.1.</span> <span class="nav-text">TensorFlow简介</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#损失计算"><span class="nav-number">5.1.1.</span> <span class="nav-text">损失计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#one-hot-encoding"><span class="nav-number">5.1.2.</span> <span class="nav-text">one_hot encoding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#实现TensorFlow-model的步骤"><span class="nav-number">5.1.3.</span> <span class="nav-text">实现TensorFlow model的步骤</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#初始化参数的方法"><span class="nav-number">5.2.</span> <span class="nav-text">初始化参数的方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播的方法"><span class="nav-number">5.3.</span> <span class="nav-text">反向传播的方法</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jeffrey Pacino</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  <script type="text/javascript" src="//cdn.jsdelivr.net/jquery/2.1.3/jquery.min.js"></script>

  
  <script type="text/javascript" src="//cdn.jsdelivr.net/fastclick/1.0.6/fastclick.min.js"></script>

  
  <script type="text/javascript" src="//cdn.jsdelivr.net/jquery.lazyload/1.9.3/jquery.lazyload.min.js"></script>

  
  <script type="text/javascript" src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.min.js"></script>

  
  <script type="text/javascript" src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.ui.min.js"></script>

  
  <script type="text/javascript" src="//cdn.jsdelivr.net/fancybox/2.1.5/jquery.fancybox.pack.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  


  




	





  





  





  






  




  
  
  
  <link rel="stylesheet" href="/vendors/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/vendors/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.0"></script>



  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

</body>
</html>
