---
title: 机器学习知识复习
mathjax: true
date: 2019-02-25 09:54:28
tags: [机器学习,深度学习]
category: [机器学习,深度学习]
---

## 机器学习

### 机器学习基础

#### 偏差与方差

- 偏差.

  偏差度量了学习**算法预测的期望值**与**真实结果**的偏离程度, 即 **刻画了学习算法本身的拟合能力** .

- 方差.

  方差表示**模型预测的期望值**与**预测值**之间的平方和，度量了同样大小的训练集的变动所导致的学习性能的变化, 即 **刻画了数据扰动所造成的影响** .

- 噪声.

  噪声表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界, 即 **刻画了学习问题本身的难度** . 巧妇难为无米之炊, 给一堆很差的食材, 要想做出一顿美味, 肯定是很有难度的.

#### 导致偏差和方差的原因

- 偏差

  通常是由于我们对学习算法做了错误的假设，或者模型的复杂度不够；

  - 比如真实模型是一个二次函数，而我们假设模型为一次函数，这就会导致偏差的增大（欠拟合）；
  - **由偏差引起的误差**通常在**训练误差**上就能体现，或者说训练误差主要是由偏差造成的

- 方差

  通常是由于模型的复杂度相对于训练集过高导致的；

  - 比如真实模型是一个简单的二次函数，而我们假设模型是一个高次函数，这就会导致方差的增大（过拟合）；
  - **由方差引起的误差**通常体现在测试误差相对训练误差的**增量**上。

### 深度学习中的偏差与方差

- 神经网络的拟合能力非常强，因此它的**训练误差**（偏差）通常较小；

- 但是过强的拟合能力会导致较大的方差，使模型的测试误差（**泛化误差**）增大；

- 因此深度学习的核心工作之一就是研究如何降低模型的泛化误差，这类方法统称为

  **正则化方法**

  1. **batch normalization**

     **1.1. BN的作用**

     * BN 是一种正则化方法（减少泛化误差），主要作用有：
       - **加速网络的训练**（缓解梯度消失，支持更大的学习率）
       - **防止过拟合**
       - 降低了**参数初始化**的要求。

     **1.2. 为什么要用BN?**

     **训练的本质是学习数据分布**。如果训练数据与测试数据的分布不同会**降低**模型的**泛化能力**。因此，应该在开始训练前对所有输入数据做归一化处理。

     而在神经网络中，因为**每个隐层**的参数不同，会使下一层的输入发生变化，从而导致每一批数据的分布也发生改变；**致使**网络在每次迭代中都需要拟合不同的数据分布，增大了网络的训练难度与**过拟合**的风险。

     **1.3. BN的基本原理**

     - BN 方法会针对**每一批数据**，在**网络的每一层输入**之前增加**归一化**处理，使输入的均值为 `0`，标准差为 `1`。**目的**是将数据限制在统一的分布下。

     - 具体来说，针对每层的第 `k` 个神经元，计算**这一批数据**在第 `k` 个神经元的均值与标准差，然后将归一化后的值作为该神经元的激活值。
       $$\hat{x}_{k} \leftarrow \frac{x_{k}-\mathrm{E}\left[x_{k}\right]}{\sqrt{\operatorname{Var}\left[x_{k}\right]}}$$

     - 但同时 BN 也降低了模型的拟合能力，破坏了之前学到的**特征分布**；

     - 为了**恢复数据的原始分布**，BN 引入了一个**重构变换**来还原最优的输入数据分布

       $$y_{k} \leftarrow \gamma \hat{X}_{k}+\beta​$$

     **1.4. BN方法小结**

     BN的过程可以归纳为一个函数：

     $$\begin{aligned} \mathrm{BN}\left(\boldsymbol{x}_{i}\right) &=\gamma \hat{\boldsymbol{x}}_{i}+\beta \\ &=\gamma \frac{\boldsymbol{x}_{i}-\mathbf{E}\left[\boldsymbol{x}_{i}\right]}{\sqrt{\boldsymbol{\operatorname { V a r }}\left[\boldsymbol{x}_{i}\right]+\epsilon}}+\beta \end{aligned}​$$

     **1.5. BN在训练和测试时分别怎么做**

     - **训练时**每次会传入一批数据，做法如前述；

     - 当**测试**或**预测时**，每次可能只会传入**单个数据**，此时模型会使用**全局统计量**代替批统计量；

       - 训练每个 batch 时，都会得到一组`（均值，方差）`；

       - 所谓全局统计量，就是对这些均值和方差求其对应的数学期望；

       - 具体计算公式为： 

         $$\begin{array}{c}{\mathrm{E}[x] \leftarrow \mathrm{E}\left[\mu_{i}\right]} \\ {\operatorname{Var}[x] \leftarrow \frac{m}{m-1} \mathrm{E}\left[\sigma_{i}^{2}\right]}\end{array}$$ 

         > 其中 $μ_i$ 和 $σ_i$ 分别表示第 i 轮 batch 保存的均值和标准差；`m` 为 batch_size，系数 `m/(m-1)` 用于计算**无偏方差估计**

         >  原文称该方法为**移动平均**（moving averages）

       - 此时的BN调整为

         $$\begin{aligned} \mathrm{BN}\left(\boldsymbol{x}_{i}\right) &=\gamma \frac{\boldsymbol{x}_{i}-\mathbf{E}\left[\boldsymbol{x}_{i}\right]}{\sqrt{\operatorname{Var}\left[\boldsymbol{x}_{i}\right]+\epsilon}}+\beta \\ &=\frac{\gamma}{\sqrt{\operatorname{Var}\left[\boldsymbol{x}_{i}\right]+\epsilon}} \boldsymbol{x}_{i}+\left(\beta-\frac{\gamma \mathbf{E}\left[\boldsymbol{x}_{i}\right]}{\sqrt{\operatorname{Var}\left[\boldsymbol{x}_{i}\right]+\epsilon}}\right)\end{aligned}$$

     具体来说：BN就是

     * 在训练时用每一批数据的均值和标准差做平均得到$\hat{X}_k$，然后用

     $y_{k} \leftarrow \gamma \hat{X}_{k}+\beta$做变换得到最终需要的x，进行训练。

     * 而测试时用每批数据得到的均值方差求平均来做归一化

     

  2. **L1/L2 范数正则化**

     **2.1. L1/L2 范数的作用、异同**
     **相同点**

     * 限制模型的学习能力——通过限制参数的规模，使模型偏好于**权值较小**的目标函数，防止过拟合。

     **不同点**

     * **L1 正则化**可以产生更**稀疏的权值矩阵**，可以用于特征选择，同时一定程度上防止过拟合；**L2 正则化**主要用于防止模型过拟合
     * **L1 正则化**适用于特征之间有关联的情况；**L2 正则化**适用于特征之间没有关联的情况。

     **2.2. 为什么 L1 和 L2 正则化可以防止过拟合？**

     - L1 & L2 正则化会使模型偏好于更小的权值。
     - 更小的权值意味着**更低的模型复杂度**；添加 L1 & L2 正则化相当于为模型添加了某种**先验**，限制了参数的分布，从而降低了模型的复杂度。
     - 模型的复杂度降低，意味着模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。——直观来说，就是对训练数据的拟合刚刚好，不会过分拟合训练数据（比如异常点，噪声）——**奥卡姆剃刀原理**

     **2.3. 为什么 L1 正则化可以产生稀疏权值，而 L2 不会？**
     L1正则化和L2正则化可以看做是损失函数的惩罚项。所谓『惩罚』是指对损失函数中的某些参数做一些限制。对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）。

     L1回归的公式如下：
     $$
     J=J_{0}+\alpha \sum_{w}|w|
     $$
     其中$J_{0}$是原始的损失函数，加号后面的一项是L1正则化项，$\alpha$是正则化系数

     其中$J_0$是原始的损失函数，加号后面的一项是L1正则化项，αα是正则化系数。注意到L1正则化是权值的绝对值之和，$J$是带有绝对值符号的函数，因此$J$是不完全可微的。机器学习的任务就是要通过一些方法（比如梯度下降）求出损失函数的最小值。当我们在原始损失函数$J_0$后添加L1正则化项时，相当于对$J_0$做了一个约束。令$L=\alpha \sum_{w}|w|$，则$J=J_{0}+L$，此时我们的任务变成在L1约束下求出$J_0$取最小值的解。考虑二维的情况，即只有两个权值$w^1$和$w^2$，此时$L=\left|w^{1}\right|+\left|w^{2}\right|$对于梯度下降法，求解$J_0$的过程可以画出等值线，同时L1正则化的函数L也可以在w1w2w1w2的二维平面上画出来。如下图：

     ![](https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/20190226104434.png)

     图中等值线是$J_{0}$的等值线，黑色方形是L函数的图形。在图中，当$J_{0}$等值线与$L$图形首次相交的地方就是最优解。上图中$J_{0}$与L在L的一个顶点处相交，这个顶点就是最优解。注意到这个顶点的值是$\left(w^{1}, w^{2}\right)=(0, w)$。可以直观想象，因为L函数有很多『突出的角』（二维情况下四个，多维情况下更多），$J_{0}$与这些角接触的机率会远大于与L其它部位接触的机率，而在这些角上，会有很多权值等于0，这就是为什么L1正则化可以产生稀疏模型，进而可以用于特征选择。

     而L2正则化的公式如下：

     $$J=J_{0}+\alpha \sum_{m} w^{2}$$

     平面图如下，因为此时L2的图形是一个圆，因此两者相交的点没有0的情况，这就是L2不产生稀疏矩阵的原因。

     ![](https://github-blog-1255346696.cos.ap-beijing.myqcloud.com/20190226104909.png)

P.S. 为什么相切的点就是所求的点。我们要求$J_0$在L约束下的最小值，L是约束，那么我们就只能在下面的菱形和圆形里面取值，相切那个点。（彩色等值线是越靠近外面越大）

